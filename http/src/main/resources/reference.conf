
# NOTE: commented fields are overridden in firecloud-develop with templated values
# Fields with "replace_me" are also expected to be overridden, but need to be in
# this file to allow referential integrity.

application {
  applicationName = "leonardo"
  #leoGoogleProject = "broad-dsde-dev"
  leoServiceAccountJsonFile = "replace_me"
  leoServiceAccountEmail = "replace_me"
  leoUrlBase = "https://replace_me"
  environment = "replace_me"
  # max concurrency for blocking calls across Leo, such as GCP, other blocking http calls, file writes, etc.
  concurrency = 255
}

# Google Cloud dataproc configuration
dataproc {
  # Add more regions as necessary.
  # This list should match the list of regions in
  # https://github.com/DataBiosphere/terra-ui/blob/dev/src/components/region-common.js
  supportedRegions = [
    "northamerica-northeast1",
    "southamerica-east1",
    "us-central1",
    "us-east1",
    "us-east4",
    "us-west1",
    "us-west2",
    "us-west3",
    "us-west4",
    "europe-central2",
    "europe-north1",
    "europe-west1",
    "europe-west2",
    "europe-west3",
    "europe-west4",
    "europe-west6",
    "asia-east1",
    "asia-east2",
    "asia-northeast1",
    "asia-northeast2",
    "asia-northeast3",
    "asia-south1",
    "asia-southeast1",
    "asia-southeast2",
    "australia-southeast1",
    "northamerica-northeast2"
  ]

  defaultScopes = [
    "https://www.googleapis.com/auth/userinfo.email",
    "https://www.googleapis.com/auth/source.read_only",
    "https://www.googleapis.com/auth/logging.write",
    "https://www.googleapis.com/auth/devstorage.full_control",
    "https://www.googleapis.com/auth/userinfo.profile",
    "https://www.googleapis.com/auth/cloud.useraccounts.readonly",
    "https://www.googleapis.com/auth/bigquery"
  ]

  runtimeDefaults {
    numberOfWorkers = 0
    masterMachineType = "n1-standard-4"
    masterDiskSize = 130
    workerMachineType = "n1-standard-4"
    workerDiskSize = 150
    numberOfWorkerLocalSSDs = 0
    numberOfPreemptibleWorkers = 0
    region = "us-central1"
  }

  # Cached dataproc image used by Terra
  customDataprocImage = "projects/broad-dsp-gcr-public/global/images/leo-dataproc-image-2-1-11-debian11-2024-07-03-18-36-42"

  # The ratio of memory allocated to spark. 0.8 = 80%.
  # Hail/Spark users generally allocate 80% of the ram to the JVM.
  sparkMemoryConfigRatio = 0.8
  # This allows at minimmum 4gb to be reserved away from Hail/Spark for Jupyter/system processes
  # Jupyter has a suggested a minimum of 1.5gb, welder has a minimum of 0.5g, and we want some space for system processes.
  # This limit is used to define the max memory available to Jupyter.
  # This limit is used to define the max memory available to Jupyter.
  minimumRuntimeMemoryInGb = 4

  monitor {
    initialDelay = 30 seconds
    # Defines polling for runtime status transitions. Used commonly for all statuses.
    # Includes user script execution, if present.
    # Does not include tool checking (see below).
    pollStatus {
      initial-delay = 2 seconds
      max-attempts = 120 # 15 seconds * 120 is 30 min
      interval = 15 seconds
    }
    # Defines a timeout per status transition. If a status is not listed there is no timeout.
    # In the case of a Starting cluster, a timeout will transition it back to Stopped. Otherwise,
    # a timeout will transition it to Error status.
    statusTimeouts {
      creating = 30 minutes
      starting = 20 minutes
      deleting = 30 minutes
    }
    # Defines polling for tool checking. This is only done for Creating and Starting status transitions.
    # Tool checking is not included in the statusTimeouts above, therefore it defines its own
    # interruptAfter.
    checkTools {
      max-attempts = 75 # 8 seconds * 75 = 10 min
      interval = 8 seconds
      interruptAfter = 10 minutes
    }
  }
}

gce {
  customGceImage = "projects/broad-dsp-gcr-public/global/images/leo-gce-image-2024-05-17-20-09-42"
  userDiskDeviceName = "user-disk"
  defaultScopes = [
    "https://www.googleapis.com/auth/userinfo.email",
    "https://www.googleapis.com/auth/userinfo.profile",
    "https://www.googleapis.com/auth/logging.write",
    "https://www.googleapis.com/auth/cloud-platform"
  ]
  runtimeDefaults {
    machineType = "n1-standard-4"
    diskSize = 30 # This is default size for just user disk
    bootDiskSize = 250
    zone = "us-central1-a"
  }
  gceReservedMemory = 1g

  setMetadataPollDelay = 1 seconds
  setMetadataPollMaxAttempts = 120

  monitor {
    initialDelay = 20 seconds
    # Defines polling for runtime status transitions. Used commonly for all statuses.
    # Includes user script execution, if present.
    # Does not include tool checking (see below).
    pollStatus {
      initial-delay = 2 seconds
      max-attempts = 120 # 15 seconds * 120 is 30 min
      interval = 15 seconds
    }
    # Defines a timeout per status transition. If a status is not listed there is no timeout.
    # In the case of a Starting cluster, a timeout will transition it back to Stopped. Otherwise,
    # a timeout will transition it to Error status.
    statusTimeouts {
      creating = 30 minutes
      starting = 20 minutes
      deleting = 30 minutes
    }
    # Defines polling for tool checking. This is only done for Creating and Starting status transitions.
    # Tool checking is not included in the statusTimeouts above, therefore it defines its own
    # interruptAfter.
    checkTools {
      max-attempts = 75 # 8 seconds * 75 = 10 min
      interval = 8 seconds
      interruptAfter = 10 minutes
    }
  }
}

azure {
    hosting-mode-config{
      # If true, it is assumed that Leo is hosted on Azure and will use Azure managed identity for authentication.
      # setting default to false so current GCP hosting is the default
      enabled = false
      azure-environment = "AZURE"
      managed-identity-auth-config {
        token-scope = ".default"
        token-acquisition-timeout = 30
      }
      publisher-config {
              topic-name = "replace_me"
              connection-string = "replace_me"
              namespace = "replace_me"
            }
      subscriber-config {
              topic-name = "replace_me"
              subscription-name = "replace_me"
              connection-string = "replace_me"
              namespace = "replace_me"
            }
  }

  wsm {
    uri = "https://localhost:8000"
  }

  bpm {
    uri = "https://localhost:8000"
  }

  tdr {
    url = "https://jade.datarepo-dev.broadinstitute.org"
  }

  pubsub-handler {
    sam-url = ${sam.server}
    wsm-url = ${azure.wsm.uri}
    welder-acr-uri = "terradevacrpublic.azurecr.io/welder-server"
    welder-image-hash = ${image.welderHash}

    // These timeouts are set to take longer than the WSM job timeout with some buffer: https://github.com/DataBiosphere/terra-workspace-manager/blob/main/service/src/main/resources/application.yml#L77
    create-vm-poll-config {
      initial-delay = 2 minutes
      max-attempts = 240 # 10 seconds * 240 is 40 min, we wait for wsm to error if at all possible (which is at 30min)
      interval = 10 seconds
    }
    delete-disk-poll-config {
      initial-delay = 1 minute
      max-attempts = 240 # 1 + 240*10 sec = 40 min
      interval = 10 seconds
    }
    delete-vm-poll-config {
      initial-delay = 30 seconds
      max-attempts = 240 # 10 seconds * 240 is 40 min
      interval = 10 seconds
    }
    start-stop-vm-poll-config {
      initial-delay = 10 seconds
      max-attempts = 30 # (10 seconds * 30 + 5) = 5:10
      interval = 10 seconds
    }
    create-disk-poll-config {
      initial-delay = 5 seconds
      max-attempts = 20 # 5 seconds * 10 + 5 is 55 sec
      interval = 5 seconds
    }
    delete-storage-container-poll-config {
      initial-delay = 5 seconds
      max-attempts = 20 # 5 seconds * 10 + 5 is 55 sec
      interval = 5 seconds
    }


    runtime-defaults {
      ip-name-prefix = "ip"
      ip-controlled-resource-desc = "Azure Ip"
      network-controlled-resource-desc = "Azure Network"
      network-name-prefix = "network"
      subnet-name-prefix = "subnet"
      address-space-cidr = "192.168.0.0/16"
      subnet-address-cidr = "192.168.0.0/24"
      disk-controlled-resource-desc = "Azure Disk"
      vm-controlled-resource-desc = "Azure Vm"
      image {
        publisher = "microsoft-dsvm"
        offer = "ubuntu-2004"
        sku = "2004-gen2"
        version = "23.04.24"
      }
      custom-script-extension {
        name = "vm-custom-script-extension",
        publisher = "Microsoft.Azure.Extensions",
        type = "CustomScript",
        version = "2.1",
        minor-version-auto-upgrade = true,
        file-uris = ["https://raw.githubusercontent.com/DataBiosphere/leonardo/8390d25ccd761fb206cf388560a571be77a42bbd/http/src/main/resources/init-resources/azure_vm_init_script.sh"]
      }
      listener-image = "terradevacrpublic.azurecr.io/terra-azure-relay-listeners:76d982c"
    }
  }


  # We need the leo azure entity for this
  app-registration {
    client-id = ""
    client-secret = ""
    managed-app-tenant-id = ""
  }

   coa-app-config {
     instrumentation-enabled = false
     chart-name = "cromwell-helm/cromwell-on-azure"
     chart-version = "0.2.523"
     release-name-suffix = "coa-rls"
     namespace-name-suffix = "coa-ns"
     ksa-name = "coa-ksa"
     dockstore-base-url = "https://staging.dockstore.org/"
     # See https://github.com/broadinstitute/cromwhelm/blob/main/terra-batch-libchart/templates/_reverse-proxy.tpl#L81-L114
     # for list of services.
     services = [
       {
         name = "cbas"
         kind = "ClusterIP"
       },
       {
         name = "cromwell"
         kind = "ClusterIP"
       }
     ]
     enabled = true
     database-enabled = false
     # App developers - Please keep the list of non-backward compatible versions in the list below
     # All app versions excluded up until the switch to WSM-controlled KubernetesNamespace
     # https://github.com/DataBiosphere/leonardo/pull/3666
     chart-versions-to-exclude-from-updates = [
       "0.2.341",
       "0.2.338",
       "0.2.334",
       "0.2.332",
       "0.2.328",
       "0.2.291",
       "0.2.277",
       "0.2.276",
       "0.2.268",
       "0.2.265",
       "0.2.263",
       "0.2.251",
       "0.2.242",
       "0.2.239",
       "0.2.237",
       "0.2.232",
       "0.2.231",
       "0.2.229",
       "0.2.225",
       "0.2.223",
       "0.2.220",
       "0.2.219",
       "0.2.218",
       "0.2.217",
       "0.2.216",
       "0.2.215",
       "0.2.213",
       "0.2.212",
       "0.2.211",
       "0.2.210",
       "0.2.209",
       "0.2.204",
       "0.2.201",
       "0.2.199",
       "0.2.197",
       "0.2.195",
       "0.2.192",
       "0.2.191",
       "0.2.187",
       "0.2.184",
       "0.2.179",
       "0.2.160",
       "0.2.159",
       "0.2.148",
       "0.2.39"
     ]
   }

  workflows-app-config {
    instrumentation-enabled = false
    chart-name = "terra-helm/workflows-app"
    chart-version = "0.253.0"
    release-name-suffix = "wfa-rls"
    namespace-name-suffix = "wfa-ns"
    ksa-name = "wfa-ksa"
    dockstore-base-url = "https://staging.dockstore.org/"
    services = [
      {
        name = "cbas"
        kind = "ClusterIP"
      },
      {
        name = "cromwell-reader"
        kind = "ClusterIP"
        path = "/cromwell"
      }
    ]
    enabled = true
    chart-versions-to-exclude-from-updates = []
    ecm-base-uri = "https://externalcreds.dsde-dev.broadinstitute.org"
    bard-base-uri = "https://terra-bard-dev.appspot.com"
    bard-enabled = false
  }

  cromwell-runner-app-config {
    instrumentation-enabled = false
    chart-name = "terra-helm/cromwell-runner-app"
    chart-version = "0.168.0"
    release-name-suffix = "cra-rls"
    namespace-name-suffix = "cra-ns"
    ksa-name = "cra-ksa"
    # See https://github.com/broadinstitute/cromwhelm/blob/main/terra-batch-libchart/templates/_reverse-proxy.tpl#L81-L114
    # for list of services.
    services = [
      {
        name = "cromwell-runner"
        kind = "ClusterIP"
        path = "/cromwell"
      }
    ]
    enabled = true
    chart-versions-to-exclude-from-updates = []
    ecm-base-uri = "https://externalcreds.dsde-dev.broadinstitute.org"
    bard-base-uri = "https://terra-bard-dev.appspot.com"
    bard-enabled = false
  }

   wds-app-config {
     environment = "dev"
     environment-base = "live"
     instrumentation-enabled = false
     chart-name = "/leonardo/wds"
     chart-version = "0.94.0"
     release-name-suffix = "wds-rls"
     namespace-name-suffix = "wds-ns"
     ksa-name = "wds-ksa"
     services = [
       {
         name = "wds"
         kind = "ClusterIP"
         path = "/"
       }
     ]
     enabled = true
     database-enabled = false
     # App developers - Please keep the list of non-backward compatible versions in the list below
     # All app versions excluded up until the switch to WSM-controlled KubernetesNamespace
     # https://github.com/DataBiosphere/leonardo/pull/3666
     chart-versions-to-exclude-from-updates = [
       "0.3.0",
       "0.7.0",
       "0.13.0",
       "0.16.0",
       "0.17.0",
       "0.19.0",
       "0.20.0",
       "0.21.0",
       "0.22.0",
       "0.24.0",
       "0.26.0",
       "0.27.0",
       "0.28.0",
       "0.31.0",
       "0.38.0",
       "0.39.0",
       "0.41.0",
       "0.42.0",
       "0.43.0"
     ]
   }

   hail-batch-app-config {
     chart-name = "/leonardo/hail-batch-terra-azure"
     chart-version = "0.2.0"
     release-name-suffix = "hail-rls"
     namespace-name-suffix = "hail-ns"
     ksa-name = "hail-ksa"
     services = [
       {
         name = "batch"
         kind = "ClusterIP"
       }
     ]
     enabled = true
     # App developers - Please keep the list of non-backward compatible versions in the list below
     chart-versions-to-exclude-from-updates = []
   }

   # App types which are allowed to launch with WORKSPACE_SHARED access scope.
   allowed-shared-apps = [
     "WDS",
     "WORKFLOWS_APP"
   ]

  listener-chart-config {
    chart-name = "terra-helm/listener"
    chart-version = "0.3.0"
  }
}

dateAccessedUpdater {
  interval = 30 seconds
  maxUpdate = 400
  queueSize = 5000
}

vpc {
  highSecurityProjectNetworkLabel = "vpc-network-name"
  highSecurityProjectSubnetworkLabel = "vpc-subnetwork-name"
  firewallAllowHttpsLabelKey = "leonardo-allow-https-firewall-name"
  firewallAllowInternalLabelKey = "leonardo-allow-internal-firewall-name"
  networkName = "leonardo-network"
  networkTag = "leonardo"
  privateAccessNetworkTag = "leonardo-private"
  # Using manual subnet creation mode because we currently only care about 1 region (us-central1)
  # and this allows us to have more control over address space. If/when we support multiple regions
  # consider auto-mode subnet creation.
  # See: https://cloud.google.com/vpc/docs/vpc#auto-mode-considerations
  autoCreateSubnetworks = false
  # Note the below 2 fields are not used if autoCreateSubnetworks is true
  subnetworkName = "leonardo-subnetwork"
  subnetworkRegionIpRangeMap = {
    us-central1 = "10.1.0.0/20"
    northamerica-northeast1 = "10.2.0.0/20"
    southamerica-east1 = "10.3.0.0/20"
    us-east1 = "10.4.0.0/20"
    us-east4 = "10.5.0.0/20"
    us-west1 = "10.6.0.0/20"
    us-west2 = "10.7.0.0/20"
    us-west3 = "10.8.0.0/20"
    us-west4 = "10.9.0.0/20"
    europe-central2 = "10.10.0.0/20"
    europe-north1 = "10.11.0.0/20"
    europe-west1 = "10.12.0.0/20"
    europe-west2 = "10.13.0.0/20"
    europe-west3 = "10.14.0.0/20"
    europe-west4 = "10.15.0.0/20"
    europe-west6 = "10.16.0.0/20"
    asia-east1 = "10.17.0.0/20"
    asia-east2 = "10.18.0.0/20"
    asia-northeast1 = "10.19.0.0/20"
    asia-northeast2 = "10.20.0.0/20"
    asia-northeast3 = "10.21.0.0/20"
    asia-south1 = "10.22.0.0/20"
    asia-southeast1 = "10.23.0.0/20"
    asia-southeast2 = "10.24.0.0/20"
    australia-southeast1 = "10.25.0.0/20"
    northamerica-northeast2 = "10.26.0.0/20"
  }
  firewallsToAdd = [
    # Allows Leonardo proxy traffic on port 443
    {
      name-prefix = "leonardo-allow-https"
      # See https://github.com/DataBiosphere/terra-resource-buffer/blob/master/src/main/java/bio/terra/buffer/service/resource/flight/CreateFirewallRuleStep.java#L37
      rbs-name = "leonardo-ssl"
      sourceRanges = {
        us-central1 = ["0.0.0.0/0"]
        northamerica-northeast1 = ["0.0.0.0/0"]
        southamerica-east1 = ["0.0.0.0/0"]
        us-east1 = ["0.0.0.0/0"]
        us-east4 = ["0.0.0.0/0"]
        us-west1 = ["0.0.0.0/0"]
        us-west2 = ["0.0.0.0/0"]
        us-west3 = ["0.0.0.0/0"]
        us-west4 = ["0.0.0.0/0"]
        europe-central2 = ["0.0.0.0/0"]
        europe-north1 = ["0.0.0.0/0"]
        europe-west1 = ["0.0.0.0/0"]
        europe-west2 = ["0.0.0.0/0"]
        europe-west3 = ["0.0.0.0/0"]
        europe-west4 = ["0.0.0.0/0"]
        europe-west6 = ["0.0.0.0/0"]
        asia-east1 = ["0.0.0.0/0"]
        asia-east2 = ["0.0.0.0/0"]
        asia-northeast1 = ["0.0.0.0/0"]
        asia-northeast2 = ["0.0.0.0/0"]
        asia-northeast3 = ["0.0.0.0/0"]
        asia-south1 = ["0.0.0.0/0"]
        asia-southeast1 = ["0.0.0.0/0"]
        asia-southeast2 = ["0.0.0.0/0"]
        australia-southeast1 = ["0.0.0.0/0"]
        northamerica-northeast2 = ["0.0.0.0/0"]
      }
      allowed = [
        {
          protocol = "tcp"
          port = "443"
        }
      ]
    },
    # Allows traffic via internal IP
    # This is a requirement for Dataproc nodes to be able to communicate with each other within a cluster.
    # Values are copied from https://github.com/DataBiosphere/terra-resource-buffer/blob/master/src/main/java/bio/terra/buffer/service/resource/flight/CreateSubnetsStep.java#L53
    {
      name-prefix = "leonardo-allow-internal"
      # See https://github.com/DataBiosphere/terra-resource-buffer/blob/master/src/main/java/bio/terra/buffer/service/resource/flight/CreateFirewallRuleStep.java#L34
      rbs-name = "leonardo-allow-internal"
      sourceRanges = {
        asia-east1 = ["10.140.0.0/20"]
        asia-east2 = ["10.170.0.0/20"]
        asia-northeast1 = ["10.146.0.0/20"]
        asia-northeast2 = ["10.174.0.0/20"]
        asia-northeast3 = ["10.178.0.0/20"]
        asia-south1 = ["10.160.0.0/20"]
        asia-southeast1 = ["10.148.0.0/20"]
        asia-southeast2 = ["10.184.0.0/20"]
        australia-southeast1 = ["10.152.0.0/20"]
        europe-central2 = ["10.186.0.0/20"]
        europe-north1 = ["10.166.0.0/20"]
        europe-west1 = ["10.132.0.0/20"]
        europe-west2 = ["10.154.0.0/20"]
        europe-west3 = ["10.156.0.0/20"]
        europe-west4 = ["10.164.0.0/20"]
        europe-west6 = ["10.172.0.0/20"]
        northamerica-northeast1 = ["10.162.0.0/20"]
        northamerica-northeast2 = ["10.188.0.0/20"]
        southamerica-east1 = ["10.158.0.0/20"]
        us-central1 = ["10.128.0.0/20"]
        us-east1 = ["10.142.0.0/20"]
        us-east4 = ["10.150.0.0/20"]
        us-west1 = ["10.138.0.0/20"]
        us-west2 = ["10.168.0.0/20"]
        us-west3 = ["10.180.0.0/20"]
        us-west4 = ["10.182.0.0/20"]
      }
      allowed = [
        {
          protocol = "tcp"
          port = "0-65535"
        },
        {
          protocol = "udp"
          port = "0-65535"
        },
        {
          protocol = "icmp"
        }
      ]
    },
    # Allows SSH access from the Broad network or VPN
    # IP list obtained from https://docs.google.com/document/d/1adV0LC2f_GIpl3A1AeoQuNiwcP59NToIt6VYT3xRCkU/edit
    {
      name-prefix = "leonardo-allow-broad-ssh"
      sourceRanges = {
        us-central1 = ${gke.cluster.authorizedNetworks}
        northamerica-northeast1 = ${gke.cluster.authorizedNetworks}
        southamerica-east1 = ${gke.cluster.authorizedNetworks}
        us-east1 = ${gke.cluster.authorizedNetworks}
        us-east4 = ${gke.cluster.authorizedNetworks}
        us-west1 = ${gke.cluster.authorizedNetworks}
        us-west2 = ${gke.cluster.authorizedNetworks}
        us-west3 = ${gke.cluster.authorizedNetworks}
        us-west4 = ${gke.cluster.authorizedNetworks}
        europe-central2 = ${gke.cluster.authorizedNetworks}
        europe-north1 = ${gke.cluster.authorizedNetworks}
        europe-west1 = ${gke.cluster.authorizedNetworks}
        europe-west2 = ${gke.cluster.authorizedNetworks}
        europe-west3 = ${gke.cluster.authorizedNetworks}
        europe-west4 = ${gke.cluster.authorizedNetworks}
        europe-west6 = ${gke.cluster.authorizedNetworks}
        asia-east1 = ${gke.cluster.authorizedNetworks}
        asia-east2 = ${gke.cluster.authorizedNetworks}
        asia-northeast1 = ${gke.cluster.authorizedNetworks}
        asia-northeast2 = ${gke.cluster.authorizedNetworks}
        asia-northeast3 = ${gke.cluster.authorizedNetworks}
        asia-south1 = ${gke.cluster.authorizedNetworks}
        asia-southeast1 = ${gke.cluster.authorizedNetworks}
        asia-southeast2 = ${gke.cluster.authorizedNetworks}
        australia-southeast1 = ${gke.cluster.authorizedNetworks}
        northamerica-northeast2 = ${gke.cluster.authorizedNetworks}
      }
      allowed = [
        {
          protocol = "tcp"
          port = 22
        }
      ]
    }
    # Allows SSH access from google IAP proxy
    # IP list obtained from https://cloud.google.com/iap/docs/using-tcp-forwarding#create-firewall-rule
    {
      name-prefix = "leonardo-allow-iap-ssh"
      sourceRanges = {
        us-central1 = ["35.235.240.0/20"]
        northamerica-northeast1 = ["35.235.240.0/20"]
        southamerica-east1 = ["35.235.240.0/20"]
        us-east1 = ["35.235.240.0/20"]
        us-east4 = ["35.235.240.0/20"]
        us-west1 = ["35.235.240.0/20"]
        us-west2 = ["35.235.240.0/20"]
        us-west3 = ["35.235.240.0/20"]
        us-west4 = ["35.235.240.0/20"]
        europe-central2 = ["35.235.240.0/20"]
        europe-north1 = ["35.235.240.0/20"]
        europe-west1 = ["35.235.240.0/20"]
        europe-west2 = ["35.235.240.0/20"]
        europe-west3 = ["35.235.240.0/20"]
        europe-west4 = ["35.235.240.0/20"]
        europe-west6 = ["35.235.240.0/20"]
        asia-east1 = ["35.235.240.0/20"]
        asia-east2 = ["35.235.240.0/20"]
        asia-northeast1 = ["35.235.240.0/20"]
        asia-northeast2 = ["35.235.240.0/20"]
        asia-northeast3 = ["35.235.240.0/20"]
        asia-south1 = ["35.235.240.0/20"]
        asia-southeast1 = ["35.235.240.0/20"]
        asia-southeast2 = ["35.235.240.0/20"]
        australia-southeast1 = ["35.235.240.0/20"]
        northamerica-northeast2 = ["35.235.240.0/20"]
      }
      allowed = [
        {
          protocol = "tcp"
          port = 22
        }
      ]
    }
  ]
  # Remove RDP and SSH rules in the default network. Also remove legacy leonardo-notebooks-rule if it exists.
  firewallsToRemove = ["default-allow-rdp", "default-allow-icmp", "allow-icmp"]
  pollPeriod = 5 seconds
  maxAttempts = 24 # 2 minutes
}

groups {
  #subEmail = "google@{{$appsSubdomain}}"
  #dataprocImageProjectGroupName = "dataproc-image-project-group"
  #dataprocImageProjectGroupEmail = ${groups.dataprocImageProjectGroupName}"@{{$appsSubdomain}}"
  waitForMemberAddedPollConfig = {
    initial-delay = 5 seconds
    max-attempts = 120
    interval = 5 seconds
  }
}

gke {
  cluster {
    location = "us-central1-a",
    region = "us-central1",
    #taken from https://dsp-security.broadinstitute.org/cloud-security/google-cloud-platform/gke
    authorizedNetworks = ["69.173.127.0/25",
                          "69.173.124.0/23",
                          "69.173.126.0/24",
                          "69.173.127.230/31",
                          "69.173.64.0/19",
                          "69.173.127.224/30",
                          "69.173.127.192/27",
                          "69.173.120.0/22",
                          "69.173.127.228/32",
                          "69.173.127.232/29",
                          "69.173.127.128/26",
                          "69.173.96.0/20",
                          "69.173.127.240/28",
                          "69.173.112.0/21"
    ]
    # See https://cloud.google.com/kubernetes-engine/docs/release-notes
    version = "1.28"
    nodepoolLockCacheExpiryTime = 1 hour
    nodepoolLockCacheMaxSize = 200

    autopilot {
      welder {
        cpu = 500
        memory = 3
        ephemeral-storage = 1
      }
      wondershaper {
        cpu = 500
        memory = 3
        ephemeral-storage = 1
      }
    }
  }
  defaultNodepool {
    machineType = "n1-standard-1"
    numNodes = 1
    autoscalingEnabled = false
    maxNodepoolsPerDefaultNode = 16
  }
  galaxyNodepool {
    machineType = "n1-highmem-8"
    numNodes = 1
    autoscalingEnabled = false
    autoscalingConfig {
       autoscalingMin = 0
       autoscalingMax = 2
    }
  }
  ingress {
    namespace = "nginx"
    release = "nginx"
    chartName = "/leonardo/ingress-nginx"
    # If you change this here, be sure to update it in the dockerfile
    chartVersion = "3.23.0"
    loadBalancerService = "nginx-ingress-nginx-controller"
    values = [
      "controller.publishService.enabled=true",
      "controller.admissionWebhooks.enabled=false"
    ]
  }
  galaxyApp {
    # See comment in KubernetesServiceInterp for more context. Theoretically release names should
    # only need to be unique within a namespace, but something in the Galaxy chart requires them
    # to be unique within a cluster.
    releaseNameSuffix = "gxy-rls"
    chartName = "/leonardo/galaxykubeman"
    # If you change this here, be sure to update it in the dockerfile
    # This is galaxykubeman, which references Galaxy
    chartVersion = "2.9.0"
    namespaceNameSuffix = "gxy-ns"
    serviceAccountName = "gxy-ksa"
    # Setting uninstallKeepHistory will cause the `helm uninstall` command to keep a record of
    # the deleted release, which can make debugging and auditing easier. It should be safe for
    # our use case because we generate unique release names anyway.
    # See https://helm.sh/docs/intro/using_helm/#helm-uninstall-uninstalling-a-release
    uninstallKeepHistory = true
    services = [
      {
        name = "galaxy"
        kind = "ClusterIP"
      }
    ]
    # Templated by firecloud-develop
    postgres.password = "replace-me"
    orchUrl = "https://firecloud-orchestration.dsde-dev.broadinstitute.org/api/"
    drsUrl = ${drs.url}
    minMemoryGb = 5
    minNumOfCpus = 3
    enabled = true
    # App developers - Please keep the list of non-backward compatible versions in the list below
    chartVersionsToExcludeFromUpdates = [
      "0.7.3",
      "0.8.0",
      "1.2.0",
      "1.2.1",
      "1.2.2",
      "1.6.0",
      "1.6.1",
      "2.1.0",
      "2.4.4",
      "2.4.6",
      "2.4.7",
      "2.4.8",
      "2.4.9",
      "2.5.0",
      "2.5.1",
      "2.5.2",
      "2.8.0",
      "2.8.1",
      "2.9.0"
    ]
  }
  galaxyDisk {
    nfsPersistenceName = "nfs-disk"
    nfsMinimumDiskSizeGB = 100
    postgresPersistenceName = "postgres-disk"
    postgresDiskNameSuffix = "gxy-postres-disk"
    postgresDiskSizeGB = 10
    postgresDiskBlockSize = 4096
  }
  cromwellApp {
    # If you update the chart name or version here, make sure to also update it in the dockerfile:
    chartName = "/leonardo/cromwell"
    chartVersion = "0.2.523"
    services = [
      {
        name = "cromwell-service"
        kind = "ClusterIP"
      }
    ]
    # These appear to be currently unused, but BW-860 should be able to make use of them.
    releaseNameSuffix = "cromwell-rls"
    namespaceNameSuffix = "cromwell-ns"
    serviceAccountName = "cromwell-ksa"
    dbPassword = "replace-me"
    enabled = true
    # App developers - Please keep the list of non-backward compatible versions in the list below
    chartVersionsToExcludeFromUpdates = []
  }
  allowedApp {
    # If you update this here, be sure to update in the dockerfile
    # This is really just a prefix of the chart name. Full chart name for AOU app will be
    # this chartName in config file appended with allowedChartName from createAppRequest
    chartName = "/leonardo/"
    rstudioChartVersion = "0.12.0"
    sasChartVersion = "0.17.0"

    services = [
          {
            name = "app"
            kind = "ClusterIP"
          },
          {
            name = "welder-service"
            kind = "ClusterIP"
          }
        ]
    releaseNameSuffix = "app-rls"
    namespaceNameSuffix = "app-ns"
    serviceAccountName = "app-ksa"
    enabled = true
    # App developers - Please keep the list of non-backward compatible versions in the list below
    chartVersionsToExcludeFromUpdates = []
    numOfReplicas = 1
  }

  customApp {
    chartName = "/leonardo/terra-app"
    # If you update this here, be sure to update in the dockerfile
    chartVersion = "0.5.0"
    releaseNameSuffix = "app-rls"
    namespaceNameSuffix = "app-ns"
    serviceAccountName = "app-ksa"
    customApplicationAllowList = {
      default: [],
      highSecurity: []
    }
    enabled = true
    # App developers - Please keep the list of non-backward compatible versions in the list below
    chartVersionsToExcludeFromUpdates = []
  }
}

image {
  welderGcrUri = "us.gcr.io/broad-dsp-gcr-public/welder-server"
  welderDockerHubUri = "broadinstitute/welder-server"
  welderHash = "51ef0f4"
  jupyterImage =  "us.gcr.io/broad-dsp-gcr-public/terra-jupyter-gatk:2.3.6"
  proxyImage = "broadinstitute/openidc-proxy:2.3.1_2"
  # Note: If you update this, please also update prepare_gce_image.sh and
  # prepare-custom-leonardo-jupyter-dataproc-image.sh scripts.
  # It is not automatically updated by CI/CD.
  # Note: Since this is a GCR image crypto detection is currently disabled for AoU which requires
  # non-GCR images. To enable on AoU we would need to replicate the image in quay.io or potentially
  # ACR (Broad is deprecating use of Dockerhub).
  cryptoDetectorImage = "us.gcr.io/broad-dsp-gcr-public/cryptomining-detector:0.0.2"

  defaultJupyterUserHome = "/home/jupyter"
  jupyterContainerName = "jupyter-server"
  rstudioContainerName = "rstudio-server"
  welderContainerName = "welder-server"
  proxyContainerName = "proxy-server"
  cryptoDetectorContainerName = "cryptomining-detector"

  jupyterImageRegex = "us.gcr.io/broad-dsp-gcr-public/([a-z0-9-_]+):(.*)"
  rstudioImageRegex = "us.gcr.io/anvil-gcr-public/([a-z0-9-_]+):(.*)"
  broadDockerhubImageRegex = "broadinstitute/([a-z0-9-_]+):(.*)"
}

welder {
  # Set to deploy welder to clusters with the given label
  deployWelderLabel = "saturnVersion"

  # Set to upgrade welder on clusters with the given label
  updateWelderLabel = "saturnVersion"

  # Leo will only deploy welder to clusters created after this date.
  # Clusters created prior to this date will not have welder deployed and will have delocalization disabled.
  deployWelderCutoffDate = "2019-08-01"

  welderReservedMemory = 768m
}

# cluster scripts and config
gceClusterResources {
  initScript = "gce-init.sh"
  cloudInit = "cloud-init.yml"
  startupScript = "startup.sh"
  shutdownScript = "shutdown.sh"
  jupyterDockerCompose = "jupyter-docker-compose-gce.yaml"
  gpuDockerCompose = "gpu-docker-compose.yaml"
  rstudioDockerCompose = "rstudio-docker-compose-gce.yaml"
  proxyDockerCompose = "proxy-docker-compose-gce.yaml"
  welderDockerCompose = "welder-docker-compose-gce.yaml"
  proxySiteConf = "cluster-site-gce.conf"
  jupyterNotebookConfigUri = "jupyter_notebook_config.py"
  jupyterNotebookFrontendConfigUri = "notebook.json"
  customEnvVarsConfigUri = "custom_env_vars.env"
}

clusterResources {
  initScript = "init-actions.sh"
  startupScript = "startup.sh"
  shutdownScript = "shutdown.sh"
  jupyterDockerCompose = "jupyter-docker-compose.yaml"
  rstudioDockerCompose = "rstudio-docker-compose.yaml"
  proxyDockerCompose = "proxy-docker-compose.yaml"
  welderDockerCompose = "welder-docker-compose.yaml"
  proxySiteConf = "cluster-site.conf"
  jupyterNotebookConfigUri = "jupyter_notebook_config.py"
  jupyterNotebookFrontendConfigUri = "notebook.json"
  customEnvVarsConfigUri = "custom_env_vars.env"
}

clusterFiles {
  proxyServerCrt = "/etc/jupyter-server.crt"
  proxyServerKey = "/etc/jupyter-server.key"
  proxyRootCaPem = "/etc/rootCA.pem"
  proxyRootCaKey = "/etc/rootCA.key"
}

# The expiration is low because the IP changes when a runtime is pause/resumed,
# so we need to ensure we don't use stale cache entries.
runtimeDnsCache {
  cacheExpiryTime = 1 minute
  cacheMaxSize = 1000
}

# Kubernetes expiration can be higher because the IP is at the cluster level, which is
# consistent across app pause/resume. Clusters are garbage collected ~1 hour after app deletion.
kubernetesDnsCache {
  cacheExpiryTime = 10 minutes
  cacheMaxSize = 100
}

mysql {
  profile = "slick.jdbc.MySQLProfile$"
  batchSize = 2000
  db {
    driver = "com.mysql.cj.jdbc.Driver"
    #url = "jdbc:mysql://mysql/leonardo
    #user = "dbUser"
    #password = "pass"
    connectionTimeout = 5000
    numThreads = 200
  }
  concurrency = 120
}

# Liquibase configuration
liquibase {
  changelog = "org/broadinstitute/dsde/workbench/leonardo/liquibase/changelog.xml"
  initWithLiquibase = true
}

sam {
  server = "replace_me"
}

prometheus {
  endpointPort = 9098
}

proxy {
  # Should match the proxy wildcard cert
  #proxyDomain = ".firecloud.org"
  #proxyUrlBase = "https://leo/proxy/"
  proxyPort = 443
  dnsPollPeriod = 15 seconds
  tokenCacheExpiryTime = 60 minutes
  tokenCacheMaxSize = 5000
  internalIdCacheExpiryTime = 2 minutes
  internalIdCacheMaxSize = 500
  isProxyCookiePartitioned = false
}

# The fields here will be combined to build a Content-Security-Policy header
# in the Leo proxy response.
# See https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy
# for description of the Content-Security-Policy model.
contentSecurityPolicy {
  # frameAncestors is overridden in firecloud-develop because it's environment-specific
  frameAncestors = [
    "'none'"
  ]
  scriptSrc = [
    "'self'",
    # Note: data: is insecure but needed to support tools like plotly and facets.
    # See discussion in https://github.com/DataBiosphere/leonardo/pull/1399
    "data:",
    # Note: 'unsafe-inline' in insecure but is needed by Jupyter UI.
    # See https://broadworkbench.atlassian.net/browse/IA-1763
    "'unsafe-inline'",
    "'unsafe-eval'",
    "blob:",
    "https://esm.sh/",
    "https://cdn.skypack.dev/",
    "https://unpkg.com/",
    "https://apis.google.com",
    "https://cdn.jsdelivr.net",
    "https://cdn.pydata.org"
  ]
  styleSrc = [
    "'self'",
    "'unsafe-inline'",
    "data:",
    "https://cdn.pydata.org"
  ]
  connectSrc = [
    "'self'",
    "blob:",
    "wss://*.broadinstitute.org:*",
    "wss://notebooks.firecloud.org:*",
    "*.googleapis.com",
    "https://*.npmjs.org",
    "https://data.broadinstitute.org",
    "https://s3.amazonaws.com/igv.broadinstitute.org/",
    "https://s3.amazonaws.com/igv.org.genomes/",
    "https://igv-genepattern-org.s3.amazonaws.com/genomes/",
    "https://hgdownload.soe.ucsc.edu/",
    "https://portals.broadinstitute.org/webservices/igv/",
    "https://igv.org/genomes/",
    "https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html",
    "https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js",
    "https://*.jupyter-dev.firecloud.org",
    "https://*.jupyter-prod.firecloud.org",
    "https://igv.genepattern.org",
    "https://cdn.plot.ly",
    "https://*.tile.openstreetmap.org"
  ]
  objectSrc = [
    "'none'"
  ]
  reportUri = [
    "https://terra.report-uri.com/r/d/csp/reportOnly"
  ]
}

refererConfig {
  # list of valid hosts and enabled flag is environment specific so it's overriden in firecloud develop
  validHosts = []
  enabled = false // false for dev and QA environments, true everywhere else
  originStrict = false // true only if "*" in validHosts is to be ignored by origin validation
}

swagger {
  #googleClientId = "client_id"
  #realm = "broad-dsde-dev"
}

# akka values are not specified here because they are only picked up in the leonardo.conf

# Authorization implementation config
auth {
  providerClass = "org.broadinstitute.dsde.workbench.leonardo.auth.sam.SamAuthProvider"
  providerConfig {
    samServer = ${sam.server}
    petKeyCacheEnabled = true
    petKeyCacheExpiryTime = 24 hours
    petKeyCacheMaxSize = 10000
    notebookAuthCacheEnabled = true
    notebookAuthCacheExpiryTime = 15 minutes
    notebookAuthCacheMaxSize = 1000
    providerTimeout = 30 seconds
    customAppCreationAllowedGroup = "custom_app_users"
    sasAppCreationAllowedGroup = "sas_app_users"
  }
}

# Implement and specify a class that will provide appropriate service accounts
serviceAccounts {
  providerClass = "org.broadinstitute.dsde.workbench.leonardo.auth.sam.PetClusterServiceAccountProvider"
  providerConfig {
    leoServiceAccountJsonFile = ${application.leoServiceAccountJsonFile}
    leoServiceAccountEmail = ${application.leoServiceAccountEmail}
    samServer = ${sam.server}
    petKeyCacheExpiryTime = 24 hours
    petKeyCacheMaxSize = 10000
    providerTimeout = 30 seconds
  }
  kubeConfig {
     leoServiceAccountJsonFile = ${application.leoServiceAccountJsonFile}
     leoServiceAccountEmail = ${application.leoServiceAccountEmail}
  }
}

pubsub {
  #pubsubGoogleProject = "broad-dsde-dev"
  #topicName = "leonardo-pubsub"
  queueSize = 100
  ackDeadLine = 10 minutes // TODO change back to 5 minutes when PROD-869 is resolved

  kubernetes-monitor {
    createNodepool {
      initial-delay = 10 seconds
      max-attempts = 90 # 10 seconds * 90 is 15 min
      interval = 10 seconds
    }
    deleteNodepool {
      initial-delay = 10 seconds
      max-attempts = 90 # 10 seconds * 90 is 15 min
      interval = 10 seconds
    }
    createCluster {
      initial-delay = 30 seconds
      max-attempts = 120 # 15 seconds * 120 is 30 min
      interval = 15 seconds
    }
    deleteCluster {
      initial-delay = 30 seconds
      max-attempts = 120 # 15 seconds * 120 is 30 min
      interval = 15 seconds
    }
    createIngress {
      initial-delay = 2 seconds
      max-attempts = 100 # 3 seconds * 100 is 5 min
      interval = 3 seconds
    }
    createApp {
      interval = 15 seconds
      max-attempts = 120 # 15 seconds * 120 = 30 min
      interruptAfter = 30 minutes
    }
    deleteApp {
      initial-delay = 30 seconds
      interval = 10 seconds
      max-attempts = 120 # 10 seconds * 120 = 20 min
    }
    scalingUpNodepool {
      initial-delay = 10 seconds
      max-attempts = 90 # 10 seconds * 90 is 15 min
      interval = 10 seconds
    }
    scalingDownNodepool {
      initial-delay = 5 minutes
      max-attempts = 360 # 10 seconds * 360 is 60 min
      interval = 10 seconds
    }
    startApp {
      max-attempts = 200 # 3 seconds * 200 is 10 min
      interval = 3 seconds
      interruptAfter = 10 minutes
    }
    updateApp {
      max-attempts = 200 # 3 seconds * 200 is 10 min
      interval = 3 seconds
      interruptAfter = 10 minutes
    }
    appLiveness {
      initial-delay = 1 seconds
      max-attempts = 60 # 1 min
      interval = 1 seconds
    }
  }

  subscriber {
    concurrency = 100
    // TODO change back to 5 minutes when PROD-869 is resolved
    timeout = 595 seconds // slightly less than ackDeadline
    // initial delay before we start polling for GKE cluster creation result
    gke-cluster-creation-polling-initial-delay = 5 minutes

    persistent-disk-monitor {
      create {
        checkToolsInterruptAfter = 5
        timeoutWithSourceDiskCopyInMinutes = 20
      }
      delete {
        initial-delay = 2 seconds
        max-attempts = 5
        interval = 3 seconds
      }
      update {
        initial-delay = 2 seconds
        max-attempts = 5
        interval = 3 seconds
      }
    }
  }

  non-leo-message-subscriber {
    #subscription-name = "nonLeoMessageSubscription"
    dead-letter-topic = "leoDeadLetterTopic"
    # Topic that we publish cryptomining users to.
    # Bard and Sam will subscribe to this topic and act upon it accordingly
    terra-cryptomining-topic = "terra-cryptomining"
  }
}

autoFreeze {
  enableAutoFreeze = true
  autoFreezeAfter = 30 minutes
  autoFreezeCheckScheduler = 1 minute
  maxKernelBusyLimit = 24 hours
}

autodelete {
  autodeleteCheckInterval = 2 hours
}

jupyterConfig {
  # https://*.npmjs.org and 'unsafe-eval' needed for jupyterlab
  # https://csp-evaluator.withgoogle.com/ can be used to evaluate CSP syntax
  contentSecurityPolicy = "frame-ancestors 'self' http://localhost:3000 http://localhost:4200 https://localhost:443 *.terra.bio https://bvdp-saturn-prod.appspot.com https://bvdp-saturn-staging.appspot.com https://bvdp-saturn-perf.appspot.com https://bvdp-saturn-alpha.appspot.com https://bvdp-saturn-dev.appspot.com https://all-of-us-workbench-test.appspot.com https://test.fake-research-aou.org/ https://staging.fake-research-aou.org https://stable.fake-research-aou.org https://workbench.researchallofus.org terra.biodatacatalyst.nhlbi.nih.gov *.terra.biodatacatalyst.nhlbi.nih.gov; script-src 'self' data:text/javascript 'unsafe-inline' 'unsafe-eval' https://apis.google.com; style-src 'self' 'unsafe-inline'; connect-src 'self' wss://*.broadinstitute.org:* wss://notebooks.firecloud.org:* *.googleapis.com https://*.npmjs.org https://data.broadinstitute.org https://s3.amazonaws.com/igv.broadinstitute.org/ https://s3.amazonaws.com/igv.org.genomes/ https://igv-genepattern-org.s3.amazonaws.com/genomes/ https://hgdownload.soe.ucsc.edu/ https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js https://*.jupyter-dev.firecloud.org ,https://*.jupyter-prod.firecloud.org; object-src 'none'"
}

zombieRuntimeMonitor {
  enableZombieRuntimeMonitor = true
  pollPeriod = 4 hours
  creationHangTolerance = 1 hour
  deletionConfirmationLabelKey = "deletionConfirmed"
  concurrency = 100
}

persistent-disk {
  default-disk-size-gb = 30
  default-disk-type = "pd-standard"
  default-block-size-bytes = 4096
  default-zone = "us-central1-a"
  default-galaxy-nfsdisk-size-gb = 250

  # This lists all google folders that represent VPC-SC perimeters. This list spans all environments. There is no downside to including folder ids
  # for all envs in each env and it reduces the risks of misconfiguration due to faulty conditionals or accidental cross env communication
  dont-clone-from-these-google-folders = [
    "737976594150", # terra_dev_aou_test
    "272722258246", # terra_dev_aou_test_2
    "796892826456", # terra_dev_baseline_test

    "791559643292", # terra_perf_aou_perf
    "851853921816", # terra_perf_aou_perf_2

    "300547813290", # terra_prod_aou_prod
    "762320479256", # terra_prod_aou_prod_2
    "96867205717",  # terra_prod_aou_stable
    "973382847971", # terra_prod_aou_stable_2
    "727685043294", # terra_prod_aou_staging
    "730698401092", # terra_prod_aou_staging_2
    "1030649683602",# terra_prod_aou_preprod
    "307813759063", # terra_prod_aou_preprod_2
    "931656019211"  # terra_prod_baseline_prod
  ]
}

clusterToolMonitor {
  pollPeriod = 2 minutes
}

leonardoExecutionMode = "combined"

clusterBucket {
  # number of days the staging bucket should continue to exist after a cluster is deleted
  stagingBucketExpiration = 10 days
}

ui {
  terraLabel = "saturnAutoCreated"
  allOfUsLabel = "all-of-us"
}

async-task-processor {
  queue-bound = 500
  max-concurrent-tasks = 200
}

terra-app-setup-chart {
  # During Leonardo deployment. Leo will `helm pull` the chart locally, and then move
  # cert files into the local chart.
  chart-name = "/leonardo/terra-app-setup"
  # If you change this here, be sure to update it in the dockerfile
  chart-version = "0.1.0"
}

app-service {
  # Defaults to true, but disabled for fiab runs
  enable-custom-app-check = true
  # Defaults to true, but for Production, we'll use Consumption model (details TBD as of 8/18/2023), which doesn't require Sam group check
  enable-sas-app = false
}

drs {
  url = "https://drshub.dsde-dev.broadinstitute.org/api/v4/drs/resolve"
}

metrics {
  enabled = true
  check-interval = 5 minutes
  # If true, will include the AzureCloudContext as a metric tag for Azure runtimes/apps.
  # Normally it's best to avoid tags for high-cardinality things (like workspaceId).
  # But MRGs are fairly low cardinality, and useful to incude for public preview launch.
  include-azure-cloud-context = true
}
