
# NOTE: commented fields are overridden in firecloud-develop with templated values
# Fields with "replace_me" are also expected to be overridden, but need to be in
# this file to allow referential integrity.

application {
  applicationName = "leonardo"
  #leoGoogleProject = "broad-dsde-dev"
  leoServiceAccountJsonFile = "replace_me"
  leoServiceAccountEmail = "replace_me"
}

# Google Cloud dataproc configuration
dataproc {
  region = "us-central1"
  #zone = "us-central1-a"
  defaultScopes = [
    "https://www.googleapis.com/auth/userinfo.email",
    "https://www.googleapis.com/auth/source.read_only",
    "https://www.googleapis.com/auth/logging.write",
    "https://www.googleapis.com/auth/devstorage.full_control",
    "https://www.googleapis.com/auth/userinfo.profile",
    "https://www.googleapis.com/auth/cloud.useraccounts.readonly",
    "https://www.googleapis.com/auth/bigquery"
  ]
  runtimeDefaults {
    numberOfWorkers = 0
    masterMachineType = "n1-standard-4"
    masterDiskSize = 100
    workerMachineType = "n1-standard-4"
    workerDiskSize = 100
    numberOfWorkerLocalSSDs = 0
    numberOfPreemptibleWorkers = 0
  }
  # This image is used for legacy jupyter image where hail is compatible with earlier version of spark
  legacyCustomDataprocImage = "projects/broad-dsp-gcr-public/global/images/custom-leo-image-dataproc-1-2-79-debian9-ae1fa1e"
  customDataprocImage = "projects/broad-dsp-gcr-public/global/images/custom-leo-image-dataproc-1-4-15-debian9-ae1fa1e"

  dataprocReservedMemory = 6g

  monitor {
    initialDelay = 30 seconds
    pollingInterval = 15 seconds
    pollCheckMaxAttempts = 120 # 15 seconds * 120 is 30 min
    checkToolsInterval = 8 seconds
    checkToolsMaxAttempts = 75 # 8 seconds * 75 = 10 min
    # Defines timeouts for cluster status transitions. If a status is not listed there is no timeout.
    # In the case of a Starting cluster, a timeout will transition it back to Stopped. Otherwise,
    # a timeout will transition it to Error status.
    statusTimeouts {
      creating = 30 minutes
      starting = 20 minutes
      deleting = 30 minutes
    }
  }
}

gce {
  region = "us-central1"
  # TODO is it safe to use this zone for all GCE VMs?
  zone = "us-central1-a"
  customGceImage = "projects/broad-dsp-gcr-public/global/images/custom-leo-image-gce-debian9-ae1fa1e"
  userDiskDeviceName = "user-disk"
  defaultScopes = [
    "https://www.googleapis.com/auth/userinfo.email",
    "https://www.googleapis.com/auth/userinfo.profile",
    "https://www.googleapis.com/auth/logging.write",
    "https://www.googleapis.com/auth/cloud-platform"
  ]
  runtimeDefaults {
    machineType = "n1-standard-4"
    diskSize = 30 # This is default size for just user disk
    bootDiskSize = 50
  }
  gceReservedMemory = 1g

  monitor {
    initialDelay = 20 seconds
    pollingInterval = 15 seconds
    pollCheckMaxAttempts = 120 # 15 seconds * 120 is 30 min
    checkToolsInterval = 8 seconds
    checkToolsMaxAttempts = 75 # 8 seconds * 75 = 10 min
    # Defines timeouts for cluster status transitions. If a status is not listed there is no timeout.
    # In the case of a Starting cluster, a timeout will transition it back to Stopped. Otherwise,
    # a timeout will transition it to Error status.
    statusTimeouts {
      creating = 30 minutes
      starting = 20 minutes
      deleting = 30 minutes
    }
  }
}

dateAccessedUpdater {
  interval = 30 seconds
  maxUpdate = 200
  queueSize = 5000
}

vpc {
  highSecurityProjectNetworkLabel = "vpc-network-name"
  highSecurityProjectSubnetworkLabel = "vpc-subnetwork-name"
  networkName = "leonardo-network"
  networkTag = "leonardo"
  # Using manual subnet creation mode because we currently only care about 1 region (us-central1)
  # and this allows us to have more control over address space. If/when we support multiple regions
  # consider auto-mode subnet creation.
  # See: https://cloud.google.com/vpc/docs/vpc#auto-mode-considerations
  autoCreateSubnetworks = false
  # Note the below 3 fields are not used if autoCreateSubnetworks is true
  subnetworkName = "leonardo-subnetwork"
  subnetworkRegion = "us-central1"
  subnetworkIpRange = "10.1.0.0/20"
  firewallsToAdd = [
    # Allows Leonardo proxy traffic on port 443
    {
      name = "leonardo-allow-https"
      sourceRanges = ["0.0.0.0/0"]
      allowed = [
        {
          protocol = "tcp"
          port = "443"
        }
      ]
    },
    # Allows traffic via internal IP
    # This is a requirement for Dataproc nodes to be able to communicate with each other within a cluster.
    {
      name = "leonardo-allow-internal"
      sourceRanges = ["10.1.0.0/20"]
      allowed = [
        {
          protocol = "tcp"
          port = "0-65535"
        },
        {
          protocol = "udp"
          port = "0-65535"
        },
        {
          protocol = "icmp"
        }
      ]
    },
    # Allows SSH access from the Broad network or VPN
    # IP list obtained from https://docs.google.com/document/d/1adV0LC2f_GIpl3A1AeoQuNiwcP59NToIt6VYT3xRCkU/edit
    {
      name = "leonardo-allow-broad-ssh"
      sourceRanges= ${gke.cluster.authorizedNetworks}
      allowed = [
        {
          protocol = "tcp"
          port = 22
        }
      ]
    }
  ]
  # Remove RDP and SSH rules in the default network. Also remove legacy leonardo-notebooks-rule if it exists.
  firewallsToRemove = ["default-allow-rdp", "default-allow-icmp", "allow-icmp"]
  pollPeriod = 5 seconds
  maxAttempts = 24 # 2 minutes
}

groups {
  #subEmail = "google@{{$appsSubdomain}}"
  #dataprocImageProjectGroupName = "dataproc-image-project-group"
  #dataprocImageProjectGroupEmail = ${groups.dataprocImageProjectGroupName}"@{{$appsSubdomain}}"
}

gke {
  cluster {
    location = "us-central1-a",
    region = "us-central1",
    #taken from https://dsp-security.broadinstitute.org/cloud-security/google-cloud-platform/gke
    authorizedNetworks = ["69.173.127.0/25",
                          "69.173.124.0/23",
                          "69.173.126.0/24",
                          "69.173.127.230/31",
                          "69.173.64.0/19",
                          "69.173.127.224/30",
                          "69.173.127.192/27",
                          "69.173.120.0/22",
                          "69.173.127.228/32",
                          "69.173.127.232/29",
                          "69.173.127.128/26",
                          "69.173.96.0/20",
                          "69.173.127.240/28",
                          "69.173.112.0/21"
    ]
    # See https://cloud.google.com/kubernetes-engine/docs/release-notes
    # As of 2020-10-09, 1.16.x is the default but it does not work with the Galaxy chart.
    version = "1.15.12-gke.4000"
  }
  defaultNodepool {
    machineType = "n1-standard-1"
    numNodes = 1
    autoscalingEnabled = false
    maxNodepoolsPerDefaultNode = 16
  }
  galaxyNodepool {
    machineType = "n1-standard-8"
    numNodes = 1
    autoscalingEnabled = true
    autoscalingConfig {
       autoscalingMin = 0
       autoscalingMax = 1
    }
  }
  ingress {
    namespace = "nginx"
    release = "nginx"
    # TODO Switch to ingress-nginx because of:
    # https://artifacthub.io/packages/helm/helm-stable/nginx-ingress
    chartName = "stable/nginx-ingress"
    chartVersion = "1.41.3"
    loadBalancerService = "nginx-nginx-ingress-controller"
    values = [
      "rbac.create=true",
      "controller.publishService.enabled=true"
    ]
    secrets = [
      {
        name = "ca-secret"
        secretFiles = [
          {
            name = "ca.crt"
            path = ${clusterFiles.proxyRootCaPem}
          }
        ]
      },
      {
        name = "tls-secret"
        secretFiles = [
          {
            name = "tls.crt"
            path = ${clusterFiles.proxyServerCrt}
          },
          {
            name = "tls.key"
            path = ${clusterFiles.proxyServerKey}
          }
        ]
      }
    ]
  }
  galaxyApp {
    # See comment in KubernetesServiceInterp for more context. Theoretically release names should
    # only need to be unique within a namespace, but something in the Galaxy chart requires them
    # to be unique within a cluster.
    releaseNameSuffix = "gxy-rls"
    chartName = "galaxy/galaxykubeman"
    chartVersion = "0.7.0"
    namespaceNameSuffix = "gxy-ns"
    serviceAccountName = "gxy-ksa"
    # Setting uninstallKeepHistory will cause the `helm uninstall` command to keep a record of
    # the deleted release, which can make debugging and auditing easier. It should be safe for
    # our use case because we generate unique release names anyway.
    # See https://helm.sh/docs/intro/using_helm/#helm-uninstall-uninstalling-a-release
    uninstallKeepHistory = true
    services = [
      {
        name = "galaxy"
        kind = "ClusterIP"
      }
    ]
    # Templated by firecloud-develop
    orchUrl = "https://firecloud-orchestration.dsde-dev.broadinstitute.org/api/"
    drsUrl = "https://us-central1-broad-dsde-dev.cloudfunctions.net/martha_v3"
  }
  galaxyDisk {
    nfsPersistenceName = "nfs-disk"
    postgresPersistenceName = "postgres-disk"
    # TODO: remove post-alpha once persistence is in place for Galaxy
    postgresDiskNameSuffix = "gxy-postres-disk"
    postgresDiskSizeGB = 10
    postgresDiskBlockSize = 4096
  }
}

image {
  welderGcrUri = "us.gcr.io/broad-dsp-gcr-public/welder-server"
  welderDockerHubUri = "broadinstitute/welder-server"
  welderHash = "4d380f2"
  jupyterImage =  "us.gcr.io/broad-dsp-gcr-public/terra-jupyter-gatk:1.0.7"
  legacyJupyterImage = "us.gcr.io/broad-dsp-gcr-public/leonardo-jupyter:5c51ce6935da"
  proxyImage = "broadinstitute/openidc-proxy:2.3.1_2"

  jupyterContainerName = "jupyter-server"
  rstudioContainerName = "rstudio-server"
  welderContainerName = "welder-server"
  proxyContainerName = "proxy-server"

  jupyterImageRegex = "us.gcr.io/broad-dsp-gcr-public/([a-z0-9-_]+):(.*)"
  rstudioImageRegex = "us.gcr.io/anvil-gcr-public/([a-z0-9-_]+):(.*)"
  broadDockerhubImageRegex = "broadinstitute/([a-z0-9-_]+):(.*)"
}

welder {
  welderDisabledNotebooksDir = "/home/jupyter-user"
  welderEnabledNotebooksDir = "/home/jupyter-user/notebooks"
  # Set to deploy welder to clusters with the given label
  deployWelderLabel = "saturnVersion"

  # Set to upgrade welder on clusters with the given label
  updateWelderLabel = "saturnVersion"

  # Leo will only deploy welder to clusters created after this date.
  # Clusters created prior to this date will not have welder deployed and will have delocalization disabled.
  deployWelderCutoffDate = "2019-08-01"

  welderReservedMemory = 768m
}

# cluster scripts and config
clusterResources {
  initActionsScript = "init-actions.sh"
  gceInitScript = "gce-init.sh"
  startupScript = "startup.sh"
  shutdownScript = "shutdown.sh"
  jupyterDockerCompose = "jupyter-docker-compose.yaml"
  jupyterDockerComposeGce = "jupyter-docker-compose-gce.yaml"
  rstudioDockerCompose = "rstudio-docker-compose.yaml"
  proxyDockerCompose = "proxy-docker-compose.yaml"
  welderDockerCompose = "welder-docker-compose.yaml"
  proxySiteConf = "cluster-site.conf"
  jupyterNotebookConfigUri = "jupyter_notebook_config.py"
  jupyterNotebookFrontendConfigUri = "notebook.json"
  customEnvVarsConfigUri = "custom_env_vars.env"
}

clusterFiles {
  proxyServerCrt = "/etc/jupyter-server.crt"
  proxyServerKey = "/etc/jupyter-server.key"
  proxyRootCaPem = "/etc/rootCA.pem"
  proxyRootCaKey = "/etc/rootCA.key"
  rstudioLicenseFile = "/etc/rstudio-license-file.lic"
}

runtimeDnsCache {
  cacheExpiryTime = 5 seconds
  cacheMaxSize = 10000
}

kubernetesDnsCache {
  cacheExpiryTime = 5 seconds
  cacheMaxSize = 10000
}

mysql {
  profile = "slick.jdbc.MySQLProfile$"
  batchSize = 2000
  db {
    driver = "com.mysql.cj.jdbc.Driver"
    #url = "jdbc:mysql://mysql/leonardo
    #user = "dbUser"
    #password = "pass"
    connectionTimeout = 5000
    numThreads = 200
  }
  concurrency = 120
}

# Liquibase configuration
liquibase {
  changelog = "org/broadinstitute/dsde/workbench/leonardo/liquibase/changelog.xml"
  initWithLiquibase = true
}

sam {
  server = "replace_me"
}

proxy {
  # Should match the proxy wildcard cert
  #proxyDomain = ".firecloud.org"
  #proxyUrlBase = "https://leo/proxy/"
  proxyPort = 443
  dnsPollPeriod = 15 seconds
  tokenCacheExpiryTime = 60 minutes
  tokenCacheMaxSize = 10000
  internalIdCacheExpiryTime = 2 minutes
  internalIdCacheMaxSize = 10000
}

# The fields here will be combined to build a Content-Security-Policy header
# in the Leo proxy response.
# See https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy
# for description of the Content-Security-Policy model.
contentSecurityPolicy {
  # frameAncestors is overridden in firecloud-develop because it's environment-specific
  frameAncestors = [
    "'none'"
  ]
  scriptSrc = [
    "'self'",
    # Note: data: is insecure but needed to support tools like plotly and facets.
    # See discussion in https://github.com/DataBiosphere/leonardo/pull/1399
    "data:",
    # Note: 'unsafe-inline' in insecure but is needed by Jupyter UI.
    # See https://broadworkbench.atlassian.net/browse/IA-1763
    "'unsafe-inline'",
    "'unsafe-eval'",
    "https://apis.google.com",
    "https://cdn.jsdelivr.net",
    "https://cdn.pydata.org"
  ]
  styleSrc = [
    "'self'",
    "'unsafe-inline'",
    "data:",
    "https://cdn.pydata.org"
  ]
  connectSrc = [
    "'self'",
    "wss://*.broadinstitute.org:*",
    "wss://notebooks.firecloud.org:*",
    "*.googleapis.com",
    "https://*.npmjs.org",
    "https://data.broadinstitute.org",
    "https://s3.amazonaws.com/igv.broadinstitute.org/",
    "https://s3.amazonaws.com/igv.org.genomes/",
    "https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html",
    "https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js"
  ]
  objectSrc = [
    "'none'"
  ]
  reportUri = [
    "https://terra.report-uri.com/r/d/csp/reportOnly"
  ]
}

swagger {
  #googleClientId = "client_id"
  #realm = "broad-dsde-dev"
}

# akka values are not specified here because they are only picked up in the leonardo.conf

# Authorization implementation config
auth {
  providerClass = "org.broadinstitute.dsde.workbench.leonardo.auth.sam.SamAuthProvider"
  providerConfig {
    samServer = ${sam.server}
    petTokenCacheEnabled = true
    petTokenCacheExpiryTime = 60 minutes
    petTokenCacheMaxSize = 1000
    notebookAuthCacheEnabled = true
    notebookAuthCacheExpiryTime = 30 minutes
    notebookAuthCacheMaxSize = 1000
    providerTimeout = 30 seconds
  }
}

# Implement and specify a class that will provide appropriate service accounts
serviceAccounts {
  providerClass = "org.broadinstitute.dsde.workbench.leonardo.auth.sam.PetClusterServiceAccountProvider"
  providerConfig {
    leoServiceAccountJsonFile = ${application.leoServiceAccountJsonFile}
    leoServiceAccountEmail = ${application.leoServiceAccountEmail}
    samServer = ${sam.server}
    petTokenCacheExpiryTime = 60 minutes
    petTokenCacheMaxSize = 1000
    providerTimeout = 30 seconds
  }
  kubeConfig {
     leoServiceAccountJsonFile = ${application.leoServiceAccountJsonFile}
     leoServiceAccountEmail = ${application.leoServiceAccountEmail}
  }
}

pubsub {
  #pubsubGoogleProject = "broad-dsde-dev"
  #topicName = "leonardo-pubsub"
  queueSize = 100
  ackDeadLine = 5 minutes

  kubernetes-monitor {
    createNodepool {
      max-attempts = 90 # 10 seconds * 90 is 15 min
      interval = 10 seconds
    }
    deleteNodepool {
      max-attempts = 90 # 10 seconds * 90 is 15 min
      interval = 10 seconds
    }
    createCluster {
      max-attempts = 120 # 15 seconds * 120 is 30 min
      interval = 15 seconds
    }
    deleteCluster {
      max-attempts = 120 # 15 seconds * 120 is 30 min
      interval = 15 seconds
    }
    createIngress {
      max-attempts = 100 # 3 seconds * 100 is 5 min
      interval = 3 seconds
    }
    createApp {
      interval = 10 seconds
      max-attempts = 120 # 10 seconds * 120 = 20 min
    }
    deleteApp {
      interval = 10 seconds
      max-attempts = 120 # 10 seconds * 120 = 20 min
    }
  }

  subscriber {
    concurrency = 100
    timeout = 295 seconds // slightly less than ackDeadline

    persistent-disk-monitor {
      create {
        max-attempts = 5

        interval = 3 seconds
      }
      delete {
        max-attempts = 5
        interval = 3 seconds
      }
      update {
        max-attempts = 5
        interval = 3 seconds
      }
    }
  }
}

autoFreeze {
  enableAutoFreeze = true
  autoFreezeAfter = 30 minutes
  autoFreezeCheckScheduler = 1 minute
  maxKernelBusyLimit = 24 hours
}

jupyterConfig {
  # https://*.npmjs.org and 'unsafe-eval' needed for jupyterlab
  # https://csp-evaluator.withgoogle.com/ can be used to evaluate CSP syntax
  contentSecurityPolicy = "frame-ancestors 'self' http://localhost:3000 http://localhost:4200 https://localhost:443 *.terra.bio https://bvdp-saturn-prod.appspot.com https://bvdp-saturn-staging.appspot.com https://bvdp-saturn-perf.appspot.com https://bvdp-saturn-alpha.appspot.com https://bvdp-saturn-dev.appspot.com https://all-of-us-workbench-test.appspot.com https://all-of-us-rw-staging.appspot.com https://all-of-us-rw-stable.appspot.com https://stable.fake-research-aou.org https://workbench.researchallofus.org terra.biodatacatalyst.nhlbi.nih.gov *.terra.biodatacatalyst.nhlbi.nih.gov; script-src 'self' data:text/javascript 'unsafe-inline' 'unsafe-eval' https://apis.google.com; style-src 'self' 'unsafe-inline'; connect-src 'self' wss://*.broadinstitute.org:* wss://notebooks.firecloud.org:* *.googleapis.com https://*.npmjs.org https://data.broadinstitute.org https://s3.amazonaws.com/igv.broadinstitute.org/ https://s3.amazonaws.com/igv.org.genomes/ https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js; object-src 'none'"
}

zombieRuntimeMonitor {
  enableZombieRuntimeMonitor = true
  pollPeriod = 4 hours
  creationHangTolerance = 1 hour
  deletionConfirmationLabelKey = "deletionConfirmed"
  concurrency = 100
}

persistentDisk {
  defaultDiskSizeGB = 30
  defaultDiskType = "pd-standard"
  defaultBlockSizeBytes = 4096
  zone = "us-central1-a"
  defaultGalaxyNFSDiskSizeGB = 500
}

clusterToolMonitor {
  pollPeriod = 2 minutes
}

leonardoExecutionMode = "combined"

clusterBucket {
  # number of days the staging bucket should continue to exist after a cluster is deleted
  stagingBucketExpiration = 10 days
}

ui {
  terraLabel = "saturnAutoCreated"
  allOfUsLabel = "all-of-us"
}

async-task-processor {
  queue-bound = 500
  max-concurrent-tasks = 200
}
