
# Google Cloud dataproc configuration
dataproc {
  applicationName = "firecloud:leonardo"
  jupyterServerName = "jupyter-server"
  rstudioServerName = "rstudio-server"
  welderServerName = "welder-server"
  firewallRuleName = "leonardo-notebooks-rule"
  networkTag = "leonardo"
  projectVPCNetworkLabel = "vpc-network-name"
  projectVPCSubnetLabel = "vpc-subnetwork-name"
  defaultScopes = [
    "https://www.googleapis.com/auth/userinfo.email",
    "https://www.googleapis.com/auth/source.read_only",
    "https://www.googleapis.com/auth/logging.write",
    "https://www.googleapis.com/auth/devstorage.full_control",
    "https://www.googleapis.com/auth/userinfo.profile",
    "https://www.googleapis.com/auth/cloud.useraccounts.readonly",
    "https://www.googleapis.com/auth/bigquery"
  ],
  welderDockerImage = "us.gcr.io/broad-dsp-gcr-public/welder-server:6a783a5"
  # TODO: temp 2019-07-08
  welderDisabledNotebooksDir = "/home/jupyter-user"
  welderEnabledNotebooksDir = "/home/jupyter-user/notebooks"
  # Unset to use project-level defaults. Note a VPC/subnet with the literal name "default" may not exist.
  #vpcNetwork = "default"
  #vpcSubnet = "default"
  customDataprocImage = "projects/broad-dsp-gcr-public/global/images/custom-leo-image-08a4ad8"   # updated 8/26
  jupyterImage = "us.gcr.io/broad-dsp-gcr-public/leonardo-jupyter:5c51ce6935da"           # updated 8/26

  # Set to deploy welder to clusters with the given label
  # TODO: change this to 'saturnVersion' once ready to enable in production for Terra clusters.
  # The current value is only used for integration testing.
  deployWelderLabel = "TEST_ONLY_DEPLOY_WELDER"

  # Set to upgrade welder on clusters with the given label
  # TODO: change this to 'saturnVersion' once ready to enable in production for Terra clusters.
  # The current value is only used for integration testing.
  updateWelderLabel = "TEST_ONLY_UPDATE_WELDER"

  # Leo will prevent deploying welder to any clusters that were created before this date.
  deployWelderCutoffDate = "2019-08-01"
}

# cluster scripts and config
clusterResources {
  initActionsScript = "init-actions.sh"
  initVmScript = "init-vm.sh"
  startupScript = "startup.sh"
  jupyterDockerCompose = "jupyter-docker-compose.yaml"
  rstudioDockerCompose = "rstudio-docker-compose.yaml"
  proxyDockerCompose = "proxy-docker-compose.yaml"
  welderDockerCompose = "welder-docker-compose.yaml"
  proxySiteConf = "cluster-site.conf"
  jupyterNotebookConfigUri = "jupyter_notebook_config.py"
  jupyterNotebookFrontendConfigUri = "notebook.json"
}

clusterDefaults {
  numberOfWorkers = 0
  masterMachineType = "n1-standard-4"
  masterDiskSize = 100
  workerMachineType = "n1-standard-4"
  workerDiskSize = 100
  numberOfWorkerLocalSSDs = 0
  numberOfPreemptibleWorkers = 0
}

clusterDnsCache {
  cacheExpiryTime = 5 seconds
  cacheMaxSize = 5000
}

mysql {
  profile = "slick.jdbc.MySQLProfile$"
  batchSize = 2000
  db {
    driver = "com.mysql.cj.jdbc.Driver"
    connectionTimeout = 5000
    numThreads = 50
  }
}

# Liquibase configuration
liquibase {
  changelog = "org/broadinstitute/dsde/workbench/leonardo/liquibase/changelog.xml"
  initWithLiquibase = true
}

# Jupyter proxy server configuration
proxy {
  jupyterProxyDockerImage = "broadinstitute/openidc-proxy:2.3.1_2"
  proxyServerName = "proxy-server"
  jupyterPort = 443
  jupyterProtocol = "tcp"
  dnsPollPeriod = 15 seconds
  cacheExpiryTime = 60 minutes
  cacheMaxSize = 100
}

monitor {
  pollPeriod = 15 seconds
  maxRetries = -1  # means retry forever
  recreateCluster = true
  creatingTimeLimit = 1 hour
  startingTimeLimit = 20 minutes
  stoppingTimeLimit = 1 hour
  deletingTimeLimit = 1 hour
  updatingTimeLimit = 20 minutes
}

# akka values are not specified here because they are only picked up in the leonardo.conf

# Authorization implementation config
auth {
  providerConfig {
    # Amount of time Leo will wait for a provider response before timing out.
    # This should be set less than akka.http.server.request-timeout.
    # This option is supported for ANY provider implementation. If not specified, the default is 30 seconds.
    providerTimeout = 30 seconds
  }
}

# Implement and specify a class that will provide appropriate service accounts
serviceAccounts {
  providerConfig {
    # Amount of time Leo will wait for a provider response before timing out.
    # This should be set less than akka.http.server.request-timeout.
    # This option is supported for ANY provider implementation. If not specified, the default is 30 seconds.
    providerTimeout = 30 seconds
  }
}

autoFreeze {
  enableAutoFreeze = true
  dateAccessedMonitorScheduler = 1 minute
  autoFreezeAfter = 30 minutes
  autoFreezeCheckScheduler = 1 minute
  maxKernelBusyLimit = 24 hours
}

jupyterConfig {
  # https://*.npmjs.org and 'unsafe-eval' needed for jupyterlab
  contentSecurityPolicy = "frame-ancestors 'self' http://localhost:3000 http://localhost:4200 https://localhost:443 *.terra.bio https://bvdp-saturn-prod.appspot.com https://bvdp-saturn-staging.appspot.com https://bvdp-saturn-perf.appspot.com https://bvdp-saturn-alpha.appspot.com https://bvdp-saturn-dev.appspot.com https://all-of-us-workbench-test.appspot.com https://all-of-us-rw-staging.appspot.com https://all-of-us-rw-stable.appspot.com https://stable.fake-research-aou.org https://workbench.researchallofus.org; script-src 'self' 'unsafe-inline' 'unsafe-eval' https://apis.google.com ; style-src 'self' 'unsafe-inline'; connect-src 'self' wss://*.broadinstitute.org:* wss://notebooks.firecloud.org:* *.googleapis.com https://*.npmjs.org https://data.broadinstitute.org https://s3.amazonaws.com/igv.broadinstitute.org/ https://s3.amazonaws.com/igv.org.genomes/"
}

zombieClusterMonitor {
  enableZombieClusterMonitor = true
  pollPeriod = 30 minutes
  creationHangTolerance = 1 hour
}

clusterToolMonitor {
  pollPeriod = 2 minutes
}

leoExecutionMode {
  backLeo = true
}

clusterBucket {
  # number of days the staging bucket should continue to exist after a cluster is deleted
  stagingBucketExpiration = 10 days
}
