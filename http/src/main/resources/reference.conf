
# NOTE: commented fields are overridden in firecloud-develop with templated values
# Fields with "replace_me" are also expected to be overridden, but need to be in
# this file to allow referential integrity.

application {
  applicationName = "leonardo"
  #leoGoogleProject = "broad-dsde-dev"
  leoServiceAccountJsonFile = "replace_me"
  leoServiceAccountEmail = "replace_me"
  # max concurrency for blocking calls across Leo, such as GCP, other blocking http calls, file writes, etc.
  concurrency = 255
}

# Google Cloud dataproc configuration
dataproc {
  # add more regions as necessary
  supportedRegions = [
    "northamerica-northeast1",
    "southamerica-east1",
    "us-central1",
    "us-east1",
    "us-east4",
    "us-west1",
    "us-west2",
    "us-west3",
    "us-west4",
    "europe-central2",
    "europe-north1",
    "europe-west1",
    "europe-west2",
    "europe-west3",
    "europe-west4",
    "europe-west6",
    "asia-east1",
    "asia-east2",
    "asia-northeast1",
    "asia-northeast2",
    "asia-northeast3",
    "asia-south1",
    "asia-southeast1",
    "asia-southeast2",
    "australia-southeast1"
  ]
  defaultScopes = [
    "https://www.googleapis.com/auth/userinfo.email",
    "https://www.googleapis.com/auth/source.read_only",
    "https://www.googleapis.com/auth/logging.write",
    "https://www.googleapis.com/auth/devstorage.full_control",
    "https://www.googleapis.com/auth/userinfo.profile",
    "https://www.googleapis.com/auth/cloud.useraccounts.readonly",
    "https://www.googleapis.com/auth/bigquery"
  ]
  runtimeDefaults {
    numberOfWorkers = 0
    masterMachineType = "n1-standard-4"
    masterDiskSize = 100
    workerMachineType = "n1-standard-4"
    workerDiskSize = 100
    numberOfWorkerLocalSSDs = 0
    numberOfPreemptibleWorkers = 0
    region = "us-central1"
  }
  # This image is used for legacy jupyter image where hail is compatible with earlier version of spark
  legacyCustomDataprocImage = "projects/broad-dsp-gcr-public/global/images/custom-leo-image-dataproc-1-2-79-debian9-da32384"
  customDataprocImage = "projects/broad-dsp-gcr-public/global/images/custom-leo-image-dataproc-1-4-51-debian10-da32384"

  dataprocReservedMemory = 6g

  monitor {
    initialDelay = 30 seconds
    pollingInterval = 15 seconds
    pollCheckMaxAttempts = 120 # 15 seconds * 120 is 30 min
    checkToolsInterval = 8 seconds
    checkToolsMaxAttempts = 75 # 8 seconds * 75 = 10 min
    # Defines timeouts for cluster status transitions. If a status is not listed there is no timeout.
    # In the case of a Starting cluster, a timeout will transition it back to Stopped. Otherwise,
    # a timeout will transition it to Error status.
    statusTimeouts {
      creating = 30 minutes
      starting = 20 minutes
      deleting = 30 minutes
    }
  }
}

gce {
  customGceImage = "projects/broad-dsp-gcr-public/global/images/custom-leo-gce-cos-image-da32384"
  userDiskDeviceName = "user-disk"
  defaultScopes = [
    "https://www.googleapis.com/auth/userinfo.email",
    "https://www.googleapis.com/auth/userinfo.profile",
    "https://www.googleapis.com/auth/logging.write",
    "https://www.googleapis.com/auth/cloud-platform"
  ]
  runtimeDefaults {
    machineType = "n1-standard-4"
    diskSize = 30 # This is default size for just user disk
    bootDiskSize = 60
    zone = "us-central1-a"
  }
  gceReservedMemory = 1g

  monitor {
    initialDelay = 20 seconds
    pollingInterval = 15 seconds
    pollCheckMaxAttempts = 120 # 15 seconds * 120 is 30 min
    checkToolsInterval = 8 seconds
    checkToolsMaxAttempts = 75 # 8 seconds * 75 = 10 min
    # Defines timeouts for cluster status transitions. If a status is not listed there is no timeout.
    # In the case of a Starting cluster, a timeout will transition it back to Stopped. Otherwise,
    # a timeout will transition it to Error status.
    statusTimeouts {
      creating = 30 minutes
      starting = 20 minutes
      deleting = 30 minutes
    }
  }
}

dateAccessedUpdater {
  interval = 30 seconds
  maxUpdate = 200
  queueSize = 5000
}

vpc {
  highSecurityProjectNetworkLabel = "vpc-network-name"
  highSecurityProjectSubnetworkLabel = "vpc-subnetwork-name"
  networkName = "leonardo-network"
  networkTag = "leonardo"
  # Using manual subnet creation mode because we currently only care about 1 region (us-central1)
  # and this allows us to have more control over address space. If/when we support multiple regions
  # consider auto-mode subnet creation.
  # See: https://cloud.google.com/vpc/docs/vpc#auto-mode-considerations
  autoCreateSubnetworks = false
  # Note the below 2 fields are not used if autoCreateSubnetworks is true
  subnetworkName = "leonardo-subnetwork"
  subnetworkRegionIpRangeMap = {
    us-central1 = "10.1.0.0/20"
    northamerica-northeast1 = "10.2.0.0/20"
    southamerica-east1 = "10.3.0.0/20"
    us-east1 = "10.4.0.0/20"
    us-east4 = "10.5.0.0/20"
    us-west1 = "10.6.0.0/20"
    us-west2 = "10.7.0.0/20"
    us-west3 = "10.8.0.0/20"
    us-west4 = "10.9.0.0/20"
    europe-central2 = "10.10.0.0/20"
    europe-north1 = "10.11.0.0/20"
    europe-west1 = "10.12.0.0/20"
    europe-west2 = "10.13.0.0/20"
    europe-west3 = "10.14.0.0/20"
    europe-west4 = "10.15.0.0/20"
    europe-west6 = "10.16.0.0/20"
    asia-east1 = "10.17.0.0/20"
    asia-east2 = "10.18.0.0/20"
    asia-northeast1 = "10.19.0.0/20"
    asia-northeast2 = "10.20.0.0/20"
    asia-northeast3 = "10.21.0.0/20"
    asia-south1 = "10.22.0.0/20"
    asia-southeast1 = "10.23.0.0/20"
    asia-southeast2 = "10.24.0.0/20"
    australia-southeast1 = "10.25.0.0/20"
  }
  firewallsToAdd = [
    # Allows Leonardo proxy traffic on port 443
    {
      name-prefix = "leonardo-allow-https"
      sourceRanges = {
        us-central1 = ["0.0.0.0/0"]
        northamerica-northeast1 = ["0.0.0.0/0"]
        southamerica-east1 = ["0.0.0.0/0"]
        us-east1 = ["0.0.0.0/0"]
        us-east4 = ["0.0.0.0/0"]
        us-west1 = ["0.0.0.0/0"]
        us-west2 = ["0.0.0.0/0"]
        us-west3 = ["0.0.0.0/0"]
        us-west4 = ["0.0.0.0/0"]
        europe-central2 = ["0.0.0.0/0"]
        europe-north1 = ["0.0.0.0/0"]
        europe-west1 = ["0.0.0.0/0"]
        europe-west2 = ["0.0.0.0/0"]
        europe-west3 = ["0.0.0.0/0"]
        europe-west4 = ["0.0.0.0/0"]
        europe-west6 = ["0.0.0.0/0"]
        asia-east1 = ["0.0.0.0/0"]
        asia-east2 = ["0.0.0.0/0"]
        asia-northeast1 = ["0.0.0.0/0"]
        asia-northeast2 = ["0.0.0.0/0"]
        asia-northeast3 = ["0.0.0.0/0"]
        asia-south1 = ["0.0.0.0/0"]
        asia-southeast1 = ["0.0.0.0/0"]
        asia-southeast2 = ["0.0.0.0/0"]
        australia-southeast1 = ["0.0.0.0/0"]
      }
      allowed = [
        {
          protocol = "tcp"
          port = "443"
        }
      ]
    },
    # Allows traffic via internal IP
    # This is a requirement for Dataproc nodes to be able to communicate with each other within a cluster.
    {
      name-prefix = "leonardo-allow-internal"
      sourceRanges = {
        us-central1 = ["10.1.0.0/20"]
        northamerica-northeast1 = ["10.2.0.0/20"]
        southamerica-east1 = ["10.3.0.0/20"]
        us-east1 = ["10.4.0.0/20"]
        us-east4 = ["10.5.0.0/20"]
        us-west1 = ["10.6.0.0/20"]
        us-west2 = ["10.7.0.0/20"]
        us-west3 = ["10.8.0.0/20"]
        us-west4 = ["10.9.0.0/20"]
        europe-central2 = ["10.10.0.0/20"]
        europe-north1 = ["10.11.0.0/20"]
        europe-west1 = ["10.12.0.0/20"]
        europe-west2 = ["10.13.0.0/20"]
        europe-west3 = ["10.14.0.0/20"]
        europe-west4 = ["10.15.0.0/20"]
        europe-west6 = ["10.16.0.0/20"]
        asia-east1 = ["10.17.0.0/20"]
        asia-east2 = ["10.18.0.0/20"]
        asia-northeast1 = ["10.19.0.0/20"]
        asia-northeast2 = ["10.20.0.0/20"]
        asia-northeast3 = ["10.21.0.0/20"]
        asia-south1 = ["10.22.0.0/20"]
        asia-southeast1 = ["10.23.0.0/20"]
        asia-southeast2 = ["10.24.0.0/20"]
        australia-southeast1 = ["10.25.0.0/20"]
      }
      allowed = [
        {
          protocol = "tcp"
          port = "0-65535"
        },
        {
          protocol = "udp"
          port = "0-65535"
        },
        {
          protocol = "icmp"
        }
      ]
    },
    # Allows SSH access from the Broad network or VPN
    # IP list obtained from https://docs.google.com/document/d/1adV0LC2f_GIpl3A1AeoQuNiwcP59NToIt6VYT3xRCkU/edit
    {
      name-prefix = "leonardo-allow-broad-ssh"
      sourceRanges = {
        us-central1 = ${gke.cluster.authorizedNetworks}
        northamerica-northeast1 = ${gke.cluster.authorizedNetworks}
        southamerica-east1 = ${gke.cluster.authorizedNetworks}
        us-east1 = ${gke.cluster.authorizedNetworks}
        us-east4 = ${gke.cluster.authorizedNetworks}
        us-west1 = ${gke.cluster.authorizedNetworks}
        us-west2 = ${gke.cluster.authorizedNetworks}
        us-west3 = ${gke.cluster.authorizedNetworks}
        us-west4 = ${gke.cluster.authorizedNetworks}
        europe-central2 = ${gke.cluster.authorizedNetworks}
        europe-north1 = ${gke.cluster.authorizedNetworks}
        europe-west1 = ${gke.cluster.authorizedNetworks}
        europe-west2 = ${gke.cluster.authorizedNetworks}
        europe-west3 = ${gke.cluster.authorizedNetworks}
        europe-west4 = ${gke.cluster.authorizedNetworks}
        europe-west6 = ${gke.cluster.authorizedNetworks}
        asia-east1 = ${gke.cluster.authorizedNetworks}
        asia-east2 = ${gke.cluster.authorizedNetworks}
        asia-northeast1 = ${gke.cluster.authorizedNetworks}
        asia-northeast2 = ${gke.cluster.authorizedNetworks}
        asia-northeast3 = ${gke.cluster.authorizedNetworks}
        asia-south1 = ${gke.cluster.authorizedNetworks}
        asia-southeast1 = ${gke.cluster.authorizedNetworks}
        asia-southeast2 = ${gke.cluster.authorizedNetworks}
        australia-southeast1 = ${gke.cluster.authorizedNetworks}
      }
      allowed = [
        {
          protocol = "tcp"
          port = 22
        }
      ]
    }
  ]
  # Remove RDP and SSH rules in the default network. Also remove legacy leonardo-notebooks-rule if it exists.
  firewallsToRemove = ["default-allow-rdp", "default-allow-icmp", "allow-icmp"]
  pollPeriod = 5 seconds
  maxAttempts = 24 # 2 minutes
}

groups {
  #subEmail = "google@{{$appsSubdomain}}"
  #dataprocImageProjectGroupName = "dataproc-image-project-group"
  #dataprocImageProjectGroupEmail = ${groups.dataprocImageProjectGroupName}"@{{$appsSubdomain}}"
}

gke {
  cluster {
    location = "us-central1-a",
    region = "us-central1",
    #taken from https://dsp-security.broadinstitute.org/cloud-security/google-cloud-platform/gke
    authorizedNetworks = ["69.173.127.0/25",
                          "69.173.124.0/23",
                          "69.173.126.0/24",
                          "69.173.127.230/31",
                          "69.173.64.0/19",
                          "69.173.127.224/30",
                          "69.173.127.192/27",
                          "69.173.120.0/22",
                          "69.173.127.228/32",
                          "69.173.127.232/29",
                          "69.173.127.128/26",
                          "69.173.96.0/20",
                          "69.173.127.240/28",
                          "69.173.112.0/21"
    ]
    # See https://cloud.google.com/kubernetes-engine/docs/release-notes
    version = "1.18"
    nodepoolLockCacheExpiryTime = 1 hour
    nodepoolLockCacheMaxSize = 200
  }
  defaultNodepool {
    machineType = "n1-standard-1"
    numNodes = 1
    autoscalingEnabled = false
    maxNodepoolsPerDefaultNode = 16
  }
  galaxyNodepool {
    machineType = "n1-highmem-8"
    numNodes = 1
    autoscalingEnabled = false
    autoscalingConfig {
       autoscalingMin = 0
       autoscalingMax = 2
    }
  }
  ingress {
    namespace = "nginx"
    release = "nginx"
    chartName = "/leonardo/ingress-nginx"
    # If you change this here, be sure to update it in the dockerfile
    chartVersion = "3.23.0"
    loadBalancerService = "nginx-ingress-nginx-controller"
    values = [
      "controller.publishService.enabled=true",
      "controller.admissionWebhooks.enabled=false"
    ]
  }
  galaxyApp {
    # See comment in KubernetesServiceInterp for more context. Theoretically release names should
    # only need to be unique within a namespace, but something in the Galaxy chart requires them
    # to be unique within a cluster.
    releaseNameSuffix = "gxy-rls"
    chartName = "/leonardo/galaxykubeman"
    # If you change this here, be sure to update it in the dockerfile
    # This is galaxykubeman 1.1.0, which references Galaxy 21.05
    chartVersion = "1.1.0"
    namespaceNameSuffix = "gxy-ns"
    serviceAccountName = "gxy-ksa"
    # Setting uninstallKeepHistory will cause the `helm uninstall` command to keep a record of
    # the deleted release, which can make debugging and auditing easier. It should be safe for
    # our use case because we generate unique release names anyway.
    # See https://helm.sh/docs/intro/using_helm/#helm-uninstall-uninstalling-a-release
    uninstallKeepHistory = true
    services = [
      {
        name = "galaxy"
        kind = "ClusterIP"
      }
    ]
    # Templated by firecloud-develop
    postgres.password = "replace-me"
    orchUrl = "https://firecloud-orchestration.dsde-dev.broadinstitute.org/api/"
    drsUrl = "https://us-central1-broad-dsde-dev.cloudfunctions.net/martha_v3"
  }
  galaxyDisk {
    nfsPersistenceName = "nfs-disk"
    nfsMinimumDiskSizeGB = 100
    postgresPersistenceName = "postgres-disk"
    postgresDiskNameSuffix = "gxy-postres-disk"
    postgresDiskSizeGB = 10
    postgresDiskBlockSize = 4096
  }
  customApp {
    chartName = "/leonardo/terra-app"
    # If you update this here, be sure to update in the dockerfile
    chartVersion = "0.3.0"
    releaseNameSuffix = "rls"
  }
}

image {
  welderGcrUri = "us.gcr.io/broad-dsp-gcr-public/welder-server"
  welderDockerHubUri = "broadinstitute/welder-server"
  welderHash = "f411762"
  jupyterImage =  "us.gcr.io/broad-dsp-gcr-public/terra-jupyter-gatk:1.1.3"
  legacyJupyterImage = "us.gcr.io/broad-dsp-gcr-public/leonardo-jupyter:5c51ce6935da"
  proxyImage = "broadinstitute/openidc-proxy:2.3.1_2"
  # Note: If you update this, please also update prepare_gce_image.sh and
  # prepare-custom-leonardo-jupyter-dataproc-image.sh scripts.
  # It is not automatically updated by CI/CD.
  # Note: Since this is a GCR image crypto detection is currently disabled for AoU which requires
  # non-GCR images. To enable on AoU we would need to replicate the image in quay.io or potentially
  # ACR (Broad is deprecating use of Dockerhub).
  cryptoDetectorImage = "us.gcr.io/broad-dsp-gcr-public/cryptomining-detector:0.0.1"

  # Update this to /home/jupyter once we release new GPU images
  defaultJupyterUserHome = "/home/jupyter-user"
  jupyterContainerName = "jupyter-server"
  rstudioContainerName = "rstudio-server"
  welderContainerName = "welder-server"
  proxyContainerName = "proxy-server"
  cryptoDetectorContainerName = "cryptomining-detector"

  jupyterImageRegex = "us.gcr.io/broad-dsp-gcr-public/([a-z0-9-_]+):(.*)"
  rstudioImageRegex = "us.gcr.io/anvil-gcr-public/([a-z0-9-_]+):(.*)"
  broadDockerhubImageRegex = "broadinstitute/([a-z0-9-_]+):(.*)"
}

welder {
  # Set to deploy welder to clusters with the given label
  deployWelderLabel = "saturnVersion"

  # Set to upgrade welder on clusters with the given label
  updateWelderLabel = "saturnVersion"

  # Leo will only deploy welder to clusters created after this date.
  # Clusters created prior to this date will not have welder deployed and will have delocalization disabled.
  deployWelderCutoffDate = "2019-08-01"

  welderReservedMemory = 768m
}

# cluster scripts and config
gceClusterResources {
  initScript = "gce-init.sh"
  cloudInit = "cloud-init.yml"
  startupScript = "startup.sh"
  shutdownScript = "shutdown.sh"
  jupyterDockerCompose = "jupyter-docker-compose-gce.yaml"
  gpuDockerCompose = "gpu-docker-compose.yaml"
  rstudioDockerCompose = "rstudio-docker-compose-gce.yaml"
  proxyDockerCompose = "proxy-docker-compose-gce.yaml"
  welderDockerCompose = "welder-docker-compose-gce.yaml"
  proxySiteConf = "cluster-site-gce.conf"
  jupyterNotebookConfigUri = "jupyter_notebook_config.py"
  jupyterNotebookFrontendConfigUri = "notebook.json"
  customEnvVarsConfigUri = "custom_env_vars.env"
  cryptoDetectorDockerCompose = "crypto-detector-docker-compose-gce.yaml"
}

clusterResources {
  initScript = "init-actions.sh"
  startupScript = "startup.sh"
  shutdownScript = "shutdown.sh"
  jupyterDockerCompose = "jupyter-docker-compose.yaml"
  rstudioDockerCompose = "rstudio-docker-compose.yaml"
  proxyDockerCompose = "proxy-docker-compose.yaml"
  welderDockerCompose = "welder-docker-compose.yaml"
  proxySiteConf = "cluster-site.conf"
  jupyterNotebookConfigUri = "jupyter_notebook_config.py"
  jupyterNotebookFrontendConfigUri = "notebook.json"
  customEnvVarsConfigUri = "custom_env_vars.env"
  cryptoDetectorDockerCompose = "crypto-detector-docker-compose.yaml"
}

clusterFiles {
  proxyServerCrt = "/etc/jupyter-server.crt"
  proxyServerKey = "/etc/jupyter-server.key"
  proxyRootCaPem = "/etc/rootCA.pem"
  proxyRootCaKey = "/etc/rootCA.key"
  rstudioLicenseFile = "/etc/rstudio-license-file.lic"
}

# The expiration is low because the IP changes when a runtime is pause/resumed,
# so we need to ensure we don't use stale cache entries.
runtimeDnsCache {
  cacheExpiryTime = 5 seconds
  cacheMaxSize = 10000
}

# Kubernetes expiration can be higher because the IP is at the cluster level, which is
# consistent across app pause/resume. Clusters are garbage collected ~1 hour after app deletion.
kubernetesDnsCache {
  cacheExpiryTime = 10 minutes
  cacheMaxSize = 10000
}

mysql {
  profile = "slick.jdbc.MySQLProfile$"
  batchSize = 2000
  db {
    driver = "com.mysql.cj.jdbc.Driver"
    #url = "jdbc:mysql://mysql/leonardo
    #user = "dbUser"
    #password = "pass"
    connectionTimeout = 5000
    numThreads = 200
  }
  concurrency = 120
}

# Liquibase configuration
liquibase {
  changelog = "org/broadinstitute/dsde/workbench/leonardo/liquibase/changelog.xml"
  initWithLiquibase = true
}

sam {
  server = "replace_me"
}

proxy {
  # Should match the proxy wildcard cert
  #proxyDomain = ".firecloud.org"
  #proxyUrlBase = "https://leo/proxy/"
  proxyPort = 443
  dnsPollPeriod = 15 seconds
  tokenCacheExpiryTime = 60 minutes
  tokenCacheMaxSize = 20000
  internalIdCacheExpiryTime = 2 minutes
  internalIdCacheMaxSize = 20000
}

# The fields here will be combined to build a Content-Security-Policy header
# in the Leo proxy response.
# See https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy
# for description of the Content-Security-Policy model.
contentSecurityPolicy {
  # frameAncestors is overridden in firecloud-develop because it's environment-specific
  frameAncestors = [
    "'none'"
  ]
  scriptSrc = [
    "'self'",
    # Note: data: is insecure but needed to support tools like plotly and facets.
    # See discussion in https://github.com/DataBiosphere/leonardo/pull/1399
    "data:",
    # Note: 'unsafe-inline' in insecure but is needed by Jupyter UI.
    # See https://broadworkbench.atlassian.net/browse/IA-1763
    "'unsafe-inline'",
    "'unsafe-eval'",
    "https://apis.google.com",
    "https://cdn.jsdelivr.net",
    "https://cdn.pydata.org"
  ]
  styleSrc = [
    "'self'",
    "'unsafe-inline'",
    "data:",
    "https://cdn.pydata.org"
  ]
  connectSrc = [
    "'self'",
    "wss://*.broadinstitute.org:*",
    "wss://notebooks.firecloud.org:*",
    "*.googleapis.com",
    "https://*.npmjs.org",
    "https://data.broadinstitute.org",
    "https://s3.amazonaws.com/igv.broadinstitute.org/",
    "https://s3.amazonaws.com/igv.org.genomes/",
    "https://portals.broadinstitute.org/webservices/igv/",
    "https://igv.org/genomes/",
    "https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html",
    "https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js"
  ]
  objectSrc = [
    "'none'"
  ]
  reportUri = [
    "https://terra.report-uri.com/r/d/csp/reportOnly"
  ]
}

refererConfig {
  # list of valid hosts and enabled flag is environment specific so it's overriden in firecloud develop
  validHosts = []
  enabled = false // false for dev and QA environments, true everywhere else
}

swagger {
  #googleClientId = "client_id"
  #realm = "broad-dsde-dev"
}

# akka values are not specified here because they are only picked up in the leonardo.conf

# Authorization implementation config
auth {
  providerClass = "org.broadinstitute.dsde.workbench.leonardo.auth.sam.SamAuthProvider"
  providerConfig {
    samServer = ${sam.server}
    petTokenCacheEnabled = true
    petTokenCacheExpiryTime = 60 minutes
    petTokenCacheMaxSize = 1000
    notebookAuthCacheEnabled = true
    notebookAuthCacheExpiryTime = 30 minutes
    notebookAuthCacheMaxSize = 1000
    providerTimeout = 30 seconds
  }
}

# Implement and specify a class that will provide appropriate service accounts
serviceAccounts {
  providerClass = "org.broadinstitute.dsde.workbench.leonardo.auth.sam.PetClusterServiceAccountProvider"
  providerConfig {
    leoServiceAccountJsonFile = ${application.leoServiceAccountJsonFile}
    leoServiceAccountEmail = ${application.leoServiceAccountEmail}
    samServer = ${sam.server}
    petTokenCacheExpiryTime = 60 minutes
    petTokenCacheMaxSize = 1000
    providerTimeout = 30 seconds
  }
  kubeConfig {
     leoServiceAccountJsonFile = ${application.leoServiceAccountJsonFile}
     leoServiceAccountEmail = ${application.leoServiceAccountEmail}
  }
}

pubsub {
  #pubsubGoogleProject = "broad-dsde-dev"
  #topicName = "leonardo-pubsub"
  queueSize = 100
  ackDeadLine = 5 minutes

  kubernetes-monitor {
    createNodepool {
      max-attempts = 90 # 10 seconds * 90 is 15 min
      interval = 10 seconds
    }
    deleteNodepool {
      max-attempts = 90 # 10 seconds * 90 is 15 min
      interval = 10 seconds
    }
    createCluster {
      max-attempts = 120 # 15 seconds * 120 is 30 min
      interval = 15 seconds
    }
    deleteCluster {
      max-attempts = 120 # 15 seconds * 120 is 30 min
      interval = 15 seconds
    }
    createIngress {
      max-attempts = 100 # 3 seconds * 100 is 5 min
      interval = 3 seconds
    }
    createApp {
      interval = 10 seconds
      max-attempts = 120 # 10 seconds * 120 = 20 min
      interruptAfter = 20 minutes
    }
    deleteApp {
      interval = 10 seconds
      max-attempts = 120 # 10 seconds * 120 = 20 min
    }
    scaleNodepool {
      max-attempts = 90 # 10 seconds * 90 is 15 min
      interval = 10 seconds
    }
    setNodepoolAutoscaling {
      max-attempts = 90 # 10 seconds * 90 is 15 min
      interval = 10 seconds
    }
    startApp {
      max-attempts = 200 # 3 seconds * 200 is 10 min
      interval = 3 seconds
      interruptAfter = 10 minutes
    }
  }

  subscriber {
    concurrency = 100
    timeout = 295 seconds // slightly less than ackDeadline

    persistent-disk-monitor {
      create {
        max-attempts = 5

        interval = 3 seconds
      }
      delete {
        max-attempts = 5
        interval = 3 seconds
      }
      update {
        max-attempts = 5
        interval = 3 seconds
      }
    }
  }

  non-leo-message-subscriber {
    # subscription-name = "nonLeoMessageSubscription"
    dead-letter-topic = "leoDeadLetterTopic"
    # Topic that we publish cryptomining users to.
    # Bard and Sam will subscribe to this topic and act upon it accordingly
    terra-cryptomining-topic = "terra-cryptomining"
  }
}

autoFreeze {
  enableAutoFreeze = true
  autoFreezeAfter = 30 minutes
  autoFreezeCheckScheduler = 1 minute
  maxKernelBusyLimit = 24 hours
}

jupyterConfig {
  # https://*.npmjs.org and 'unsafe-eval' needed for jupyterlab
  # https://csp-evaluator.withgoogle.com/ can be used to evaluate CSP syntax
  contentSecurityPolicy = "frame-ancestors 'self' http://localhost:3000 http://localhost:4200 https://localhost:443 *.terra.bio https://bvdp-saturn-prod.appspot.com https://bvdp-saturn-staging.appspot.com https://bvdp-saturn-perf.appspot.com https://bvdp-saturn-alpha.appspot.com https://bvdp-saturn-dev.appspot.com https://all-of-us-workbench-test.appspot.com https://all-of-us-rw-staging.appspot.com https://all-of-us-rw-stable.appspot.com https://stable.fake-research-aou.org https://workbench.researchallofus.org terra.biodatacatalyst.nhlbi.nih.gov *.terra.biodatacatalyst.nhlbi.nih.gov; script-src 'self' data:text/javascript 'unsafe-inline' 'unsafe-eval' https://apis.google.com; style-src 'self' 'unsafe-inline'; connect-src 'self' wss://*.broadinstitute.org:* wss://notebooks.firecloud.org:* *.googleapis.com https://*.npmjs.org https://data.broadinstitute.org https://s3.amazonaws.com/igv.broadinstitute.org/ https://s3.amazonaws.com/igv.org.genomes/ https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js; object-src 'none'"
}

zombieRuntimeMonitor {
  enableZombieRuntimeMonitor = true
  pollPeriod = 4 hours
  creationHangTolerance = 1 hour
  deletionConfirmationLabelKey = "deletionConfirmed"
  concurrency = 100
}

persistent-disk {
  default-disk-size-gb = 30
  default-disk-type = "pd-standard"
  default-block-size-bytes = 4096
  default-zone = "us-central1-a"
  default-galaxy-nfsdisk-size-gb = 250
}

clusterToolMonitor {
  pollPeriod = 2 minutes
}

leonardoExecutionMode = "combined"

clusterBucket {
  # number of days the staging bucket should continue to exist after a cluster is deleted
  stagingBucketExpiration = 10 days
}

ui {
  terraLabel = "saturnAutoCreated"
  allOfUsLabel = "all-of-us"
}

async-task-processor {
  queue-bound = 500
  max-concurrent-tasks = 200
}

terra-app-setup-chart {
  # During Leonardo deployment. Leo will `helm pull` the chart locally, and then move
  # cert files into the local chart.
  chart-name = "/leonardo/terra-app-setup"
  # If you change this here, be sure to update it in the dockerfile
  chart-version = "0.0.2"
}
