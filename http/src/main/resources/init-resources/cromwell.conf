
include "application.conf"

webservice {
  port = 8002
  interface = 0.0.0.0
  instance.name = "reference"
  binding-timeout = 30s
}

akka {
  actor.default-dispatcher.fork-join-executor {
    parallelism-factor = 10.0
    parallelism-max = 8
  }

  # Increased from 20s default to allow large metadata responses
  http.server.request-timeout = 55s

  # Don't warn in server logs (and thus spam Sentry) if client headers are malformed
  http.server.parsing.illegal-header-warnings = off

  http.host-connection-pool.max-open-requests = 16384
  http.host-connection-pool.max-connections = 2000
}

system {
  cromwell_id = $(clusterName)

  # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,
  # in particular clearing up all queued database writes before letting the JVM shut down.
  # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.
  # FC NOTE: reenable this when we want to actually do graceful shutdowns (not just kill the container)
  graceful-server-shutdown = false

  # Cromwell will cap the number of running workflows at N
  # Harmonized to `maxActiveWorkflowsPerServer` in Rawls. Rawls imposes an additional limit of 7,500 per user.
  max-concurrent-workflows = 1000

  # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  max-workflow-launch-count = 30

  # Number of seconds between workflow launches
  new-workflow-poll-rate = 10

  # Increased from the default number of cache read workers (25) [PROD-360]
  number-of-cache-read-workers = 50

  # Maximum scatter width per scatter node. Cromwell will fail the workflow if the scatter width goes beyond N
  # CJL: 5/21/19: Setting this low in response to https://broadworkbench.atlassian.net/browse/PROD-137.
  max-scatter-width-per-scatter = 35000

  # Total max. jobs that can be created per root workflow. If it goes beyond N, Cromwell will fail the workflow by:
  # - no longer creating new jobs
  # - let the jobs that have already been started finish, and then fail the workflow
  # CJL: 5/21/19: Setting this low as an additional response to https://broadworkbench.atlassian.net/browse/PROD-137.
  total-max-jobs-per-root-workflow = 200000

  io {
    # Throttle for GCS calls.
    # this is our quota on broad-dsde-prod
    number-of-requests = 10240611
  }

  # Maximum number of input file bytes allowed in order to read each type.
  # If exceeded a FileSizeTooBig exception will be thrown.
  # some of these upped for FC
  input-read-limits {

    lines = 10000000

    json = 10000000

    tsv = 10000000

    object = 10000000

    bool = 7

    int = 19

    float = 50

    string = 128000

    map = 128000
  }

  job-rate-control {
    # Empirically, we have seen that PAPIv2 starts having issues if we submit jobs faster than GCS can start them for a project.
    # If we do, GCS starts returning 429s which - if they all come at the same time - break PAPI.
    jobs = 10
    per = 1 second
  }

  # More info: https://docs.google.com/presentation/d/14VgNBDE8Don_oeOOPIWtJQC5OUrcpk3Vip5AQBBCSs4/edit
  hog-safety {
    # Changed in Feb 2021 for https://broadworkbench.atlassian.net/browse/BW-442
    # Allows three projects to "hog" a cromwell instance such that other projects' work does not start
    # instantly. But remember that tokens are distributed to projects in a round-robin fashion so as
    # long as there is enough churn, jobs for smaller projects should still start in relatively timely
    # fashion.
    hog-factor = 3

    # This is the name of the workflow option that will be sent via workflow the options file and used for hog factor
    #calculations.
    workflow-option = "google_project"

    # Interval between token logs in seconds.
    token-log-interval-seconds = 300
  }

  # Cache file hashes within the scope of a root workflow to prevent repeatedly requesting the
  # hashes of the same files multiple times.
  file-hash-cache = true

  # Enable the option to delete intermediate files for a workflow
  delete-workflow-files = true

  # Coordinate transactions that mutate `WORKFLOW_STORE_ENTRY` at the application level to prevent deadlocks [WA-334]
  coordinated-workflow-store-access = true
}

#workflow-options {
#  # These workflow options will be encrypted when stored in the database
#  encrypted-fields: ["user_service_account_json"]
#
#  # AES-256 key to use to encrypt the values in `encrypted-fields`
#  base64-encryption-key: "{{$commonSecrets.Data.workflow_options_encryption_key}}"
#}

// Optional call-caching configuration.
call-caching {
  # Allows re-use of existing results for jobs you've already run
  # (default: false)
  enabled = true

  # Whether to invalidate a cache result forever if we cannot reuse them. Disable this if you expect some cache copies
  # to fail for external reasons which should not invalidate the cache (e.g. auth differences between users):
  # (default: true)
  invalidate-bad-cache-results = false

  # The maximum number of failed copy attempts Cromwell will tolerate for a single job before giving up and simply
  # running the job.
  # Last changed (reduced) 2021-02-11 https://broadworkbench.atlassian.net/browse/BW-523
  max-failed-copy-attempts = 10

  # Filter call cache hits based on authorization failures copying previous call cache hits.
  # May need to be adjusted in the future.
  blacklist-cache {
    enabled = true

    groupings {
      workflow-option = "google_project"
      ttl = "1 days"
      size = 500
      concurrency = 1000
    }

    buckets {
      ttl = "1 days"
      size = 5000
      concurrency = 10000
    }

    hits {
      ttl = "1 days"
      size = 20000
      concurrency = 10000
    }

  }
}

drs {
    localization {
        # The Docker image that contains the Martha URL-resolving and localizing code.
        docker-image = "broadinstitute/cromwell-drs-localizer:65-ed067b8"
    }
}

# This overrides the Martha URL in reference.conf.
filesystems.drs.global.config.martha.url = "https://us-central1-broad-dsde-dev.cloudfunctions.net/martha_v3"

google {

  application-name = "cromwell"

  auths = [
    {
      name = "application_default"
      scheme = "application_default"
    }
  ]
}

engine {
  # This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.
  # For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.
  # If you intend to be able to run workflows with this kind of declarations:
  # workflow {
  #    String str = read_string("gs://bucket/my-file.txt")
  # }
  # You will need to provide the engine with a gcs filesystem
  # Note that the default filesystem (local) is always available.
  filesystems {
    gcs {
      auth = "application_default"
    },
    local {
        # in workbench we need to disable local to stop people from running WDLs on our Cromwell server itself
    	enabled: false
    },
    http {
        enabled: true
    },
    drs {
      auth = "application_default"
    }
  }
}

languages {

  WDL {
    versions {
      "draft-2" {
        config.caching {
          enabled: true
          ttl: 5 seconds
          size: 100
          concurrency: 9
        }
      }

      "1.0" {
        config.caching {
          enabled: true
          ttl: 5 seconds
          size: 100
          concurrency: 9
        }
      }
    }
  }

  CWL {
    versions {
      "v1.0" {
        config.enabled = false
      }
    }
  }
}

backend {
  default = "PAPIv2-beta"
  providers {

    #Line below disables the Local backend
    Local.config.root = "/dev/null"

    PAPIv2-beta {
      actor-factory = "cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"
      config {
        include required("cromwell_papi_v2_beta_config.conf")
      }
    }
    PAPIv2-CloudNAT {
      actor-factory = "cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"
      config {
        include required("cromwell_papi_v2_beta_config.conf")

        # Same as the current value in the non-Cloud NAT backends.
        concurrent-job-limit = 14400

        virtual-private-cloud {
          network-label-key = "vpc-network-name"
          subnetwork-label-key = "vpc-subnetwork-name"
          auth = "application_default"
        }
      }
    }
  }
}

services {
  MetadataService {
    config {
      # Using the default metadata-summary-refresh-interval of "1 second".

      # The default is currently 5000. Our choice used to be much higher (250k) to deal with spikes ([PROD-360]).
      # From 50-5df1e07 onwards, this only has to process summarizable metadata (< ~0.5%), so the batch doesn't need to be
      # anywhere near as high as it used to be.
      # Given the configuration setting above which turns off summarization for frontend instances this will only have
      # meaning for the summarizer instance:
      metadata-summary-refresh-limit = 5000

      #   For higher scale environments, e.g. many workflows and/or jobs, DB write performance for metadata events
      #   can improved by writing to the database in batches. Increasing this value can dramatically improve overall
      #   performance but will both lead to a higher memory usage as well as increase the risk that metadata events
      #   might not have been persisted in the event of a Cromwell crash.
      #
      #   For normal usage the default value of 200 should be fine but for larger/production environments we recommend a
      #   value of at least 500. There'll be no one size fits all number here so we recommend benchmarking performance and
      #   tuning the value to match your environment.
      db-batch-size = 1000

      #   Periodically the stored metadata events will be forcibly written to the DB regardless of if the batch size
      #   has been reached. This is to prevent situations where events wind up never being written to an incomplete batch
      #   with no new events being generated. The default value is currently 5 seconds
      db-flush-rate = 1 second

      # Kill metadata SQL queries that have run so long that the associated request has already timed out
      # Intention is to return resources to the system within a reasonable timeframe to avoid OOM incidents like PROD-194
      # See also `akka.http.server.request-timeout`
      metadata-read-query-timeout = 60s

      # Allow up to 5 million rows of METADATA_ENTRY to be compiled into metadata responses:
      metadata-read-row-number-safety-threshold = 5000000
    }
  }
  Instrumentation {
    # Default noop service - instrumentation metrics are ignored
    class = "cromwell.services.instrumentation.impl.noop.NoopInstrumentationServiceActor"

    config {
      hostname = "statsd.hostedgraphite.com"
      port = 8125
      prefix = $(clusterName)
      flush-rate = 30 seconds
    }
  }
  HealthMonitor {
    config {
      check-engine-database: true
      # The frontend instances don't conceptually need this auth, but the current un-hotfixed Cromwell 40 will error
      # if a recognized google-auth-name is not specified. Cromwell internally defaults to 'application-default', but
      # that is not a recognized auth name in FC.
      google-auth-name = application_default

      # Legacy naming: readers call themselves `frontend` in their instance type
      # FIAB is like all the instance types in one, so check everything
      check-gcs: true
      check-papi-backends: [PAPIv2-beta, PAPIv2-CloudNAT]
      gcs-bucket-to-check = $(stagingBucketName)
    }
  }
  LoadController {
    config {
      # The load controller service will periodically look at the status of various metrics its collecting and make an
      # assessment of the system's load. If necessary an alert will be sent to the rest of the system.
      # This option sets how frequently this should happen
      control-frequency = 5 seconds
    }
  }
}

database {
  # mysql example
  profile = "slick.jdbc.MySQLProfile$"
  db {
    # These are specified in this forConfig method at http://slick.lightbend.com/doc/3.0.0/api/index.html#slick.jdbc.JdbcBackend$DatabaseFactoryDef@forConfig(String,Config,Driver):Database
    driver = "com.mysql.cj.jdbc.Driver"
    url = "jdbc:mysql://cromwell-mysql:3306/cromwell?requireSSL=false&useSSL=false&rewriteBatchedStatements=true"
    user = "cromwell"
    password = "cromwell"
    connectionTimeout = 60000
    numThreads = 100
    minThreads = 100
    maxThreads = 100
    minConnections = 100
    maxConnections = 100
    queueSize = 2000
  }
}

# Configuration for load-control related values
load-control {
  ## Queue Size Thresholds ##
  # Cromwell centralizes some operations through singleton actors (possibly acting as routers).
  # This allows for a more efficient control, throttling, and potentially batching of those operations which overall improves throughput.
  # In order to do that, those operations are queued until they can be performed.
  # Those queues are for the most part unbounded in size, which can become a problem under heavy load.
  # Each actor can however let the load controller service know when it considers its work load to be abnormally high.
  # In the case of those queuing actors, this means that their queue size is over a certain threshold.
  # This section allows to configure those threshold values.
  # They should be kept at a reasonable number where reasonable will depend on your system and how much load Cromwell is submitted to.
  # If they're too high they could end up using a lot of memory, if they're too small any small spike will be considered a high load and the system will automatically slow itself down.
  # Benchmarking is recommended to find the values that suit your use case best.
  # If you use the statsD instrumentation service, the queue size and throughput of these actors are instrumented and looking at their value over time can also help you find the right configuration.

  # FC NOTE: this is only applicable for workflow restart (which only occurs on cromwell restart)
  #          we should track this when we restart cromwell (monitor queue size and throughput)
  job-store-read = 10000
  # FC NOTE: this occurs when we workflow jobs are running.  we should monitor queue size and throughput
  #          while cromwell is running to determine if we need to change this value
  job-store-write = 10000


  # call cache read actors are routed (several actors are performing cache read operations
  # this threshold applies to each routee individually, so set it to a lower value to account for that
  # to change the number of routees, see the services.number-of-cache-read-workers config value
  # FC NOTE: monitor queue size and throughput when we are seeing jobs that are slowed down by
  #          call cache lookup step
  call-cache-read = 1000
  call-cache-write = 10000

  # FC NOTE: this is storing of the papi job id only at the moment
  key-value-read = 10000
  key-value-write = 10000


  # The I/O queue has the specificity to be bounded. This sets its size
  # FC NOTE: io operations directly to google buckets (e.g. exec.sh)
  io-queue-size = 10000
  # If the I/O queue is full, subsequent requests are rejected and to be retried later.
  # This time window specifies how much time without request rejection consititutes a "back to normal load" event
  io-normal-window = 10s

  # FC NOTE: we should closely watch this one since it's a known bottleneck for FC
  # metadata is an order of magnitude higher because its normal load is much higher than other actors
  metadata-write = 100000

  # FC NOTE: at the moment memory management below isn't really working right anyway,
  #          but doesn't cause harm. leave alone for now
  # Memory is monitored by looking at the amount of free memory left
  memory-threshold-in-mb = 1024
  # Because memory measurements are not precise, the last memory-measurement-window measurements are recorded
  # and changes on load are only triggered if all measurements are below or above the threshold
  # The default value 6 combined with a monitoring frequencey of 5 seconds means that load alerts are triggered
  # If the memory is below the threshold for at least 30 consecutive seconds
  memory-measurement-window = 6

}
