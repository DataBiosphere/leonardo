# a test DB is required.  Here's an example command to run one locally

# docker run --name mysql-leonardo \
# -e MYSQL_ROOT_PASSWORD=leonardo-test \
# -e MYSQL_USER=leonardo-test \
# -e MYSQL_PASSWORD=leonardo-test \
# -e MYSQL_DATABASE=leotestdb \
# -d -p 3311:3306 mysql/mysql-server:5.6

application {
  leoGoogleProject = "leo-project"
  leoServiceAccountJsonFile = "leo-account.json"
  leoServiceAccountEmail = "leo@leo.com"
}

groups {
  subEmail = "google@dev.test.firecloud.org"
  dataprocImageProjectGroupName = "dataproc-image-project-group"
  dataprocImageProjectGroupEmail = ${groups.dataprocImageProjectGroupName}"@test.firecloud.org"
  waitForMemberAddedPollConfig = {
    initial-delay = 0 seconds
    max-attempts = 3
    interval = 1 seconds
  }
}

image {
  welderImage = "testwelderrepo/test"
  jupyterImage =  "testjupyterrepo/test"
  proxyImage = "testproxyrepo/test"
}

mysql {
  profile = "slick.jdbc.MySQLProfile$"
  batchSize = 5000
  host = "localhost"
  port = 3311
  db {
    driver = "com.mysql.cj.jdbc.Driver"
    url = "jdbc:mysql://"${mysql.host}":"${mysql.port}"/leotestdb?createDatabaseIfNotExist=true&useSSL=false&rewriteBatchedStatements=true&nullNamePatternMatchesAll=true"
    user = "leonardo-test"
    password = "leonardo-test"
    connectionTimeout = "5 seconds"  // default of 1 sec sometimes too short for docker local mysql
  }
  concurrency = 20
}

dateAccessedUpdater {
  interval = 1 seconds
  maxUpdate = 10
}

dataproc {
  monitor {
    initialDelay = 0 seconds
    pollingInterval = 1 seconds
    checkToolsDelay = 0 seconds
    pollCheckMaxAttempts = 10 # 15 seconds * 120 is 30 min
    # Defines timeouts for cluster status transitions. If a status is not listed there is no timeout.
    # In the case of a Starting cluster, a timeout will transition it back to Stopped. Otherwise,
    # a timeout will transition it to Error status.
    statusTimeouts {
      creating = 20 seconds
      starting = 20 seconds
      deleting = 20 seconds
      stopping = 20 seconds
    }
  }
}

pubsub {
  pubsubGoogleProject = "broad-dsde-dev"
  topicName = "leonardo-pubsub"
  queueSize = 100

  non-leo-message-subscriber {
    subscription-name = "nonLeoMessageSubscription"
  }

  kubernetes-monitor {
    startApp {
      max-attempts = 5
      interval = 1 seconds
    }
    updateApp {
      max-attempts = 5
      interval = 1 seconds
    }
    scalingUpNodepool {
      initial-delay = 0 seconds
      max-attempts = 10 # 10 seconds * 90 is 15 min
      interval = 2 seconds
    }
    scalingDownNodepool {
      initial-delay = 0 seconds
      max-attempts = 10 # 10 seconds * 90 is 15 min
      interval = 2 seconds
    }
    createApp {
      interval = 0 seconds
      max-attempts = 120
      interruptAfter = 30 seconds
    }
    deleteApp {
      initial-delay = 0 seconds
      interval = 0 seconds
      max-attempts = 120
    }
  }
}

clusterFiles {
  proxyServerCrt = "http/src/test/resources/test-server.crt"
  proxyServerKey = "http/src/test/resources/test-server.key"
  proxyRootCaPem = "http/src/test/resources/test-server.pem"
  proxyRootCaKey = "http/src/test/resources/test-server.key"
}

clusterResources {
  initActionsScript = "test-init-actions.sh"
  initVmScript = "init-vm.sh"
  jupyterDockerCompose = "test-jupyter-docker-compose.yaml"
  jupyterDockerComposeGce = "test-jupyter-docker-compose-gce.yaml"
  rstudioDockerCompose = "test-rstudio-docker-compose.yaml"
  proxyDockerCompose = "test-proxy-docker-compose.yaml"
  cryptoDetectorDockerCompose = "test-crypto-detector-docker-compose.yaml"
  proxySiteConf = "test-site.conf"
  jupyterNotebookConfigUri = "jupyter_notebook_config.py"
  jupyterNotebookFrontendConfigUri = "notebook.json"
  welderDockerCompose = "test-welder-docker-compose.yaml"
}

sam {
  server = "https://sam.test.org:443"
}

proxy {
  proxyDomain = ".jupyter.firecloud.org"
  proxyUrlBase = "https://leo/proxy/"
  proxyPort = 8001
}

oidc {
  authority-endpoint = "https://fake"
  client-id = "fakeClientId"
  client-secret = "fakeClientSecret"
  legacy-google-client-id = "legacyClientSecret"
}

refererConfig {
  validHosts = [
    "example.com",
    "localhost:3000"
  ]
  enabled = true
}

swagger {
  googleClientId = "test.apps.googleusercontent.com"
  realm = "broad-dsde-test"
}

monitor {
  pollPeriod = 1 second
  maxRetries = -1  # means retry forever
  recreateCluster = true
  statusTimeouts {
    creating = 5 seconds
    starting = 5 seconds
    stopping = 5 seconds
    deleting = 5 seconds
    updating = 5 seconds
  }
}

auth {
  providerClass = "org.broadinstitute.dsde.workbench.leonardo.auth.SamAuthProvider"
  providerConfig = {
    samServer = "https://sam.test.org:443"
    petTokenCacheEnabled = true
    petTokenCacheExpiryTime = "60 minutes"
    petTokenCacheMaxSize = 100
  }

  allowlistProviderConfig = {
    allowlist = ["user1@example.com", "user2@example.com", "user3@example.com", "user100@example.com", "user101@example.com", "pet-1234567890@test-project.iam.gserviceaccount.com"]
  }
  allowlistProviderConfig2 = {
    allowlist = ["user2@example.com", "user3@example.com", "user100@example.com", "user101@example.com", "pet-1234567890@test-project.iam.gserviceaccount.com"]
  }

  samAuthProviderConfig = {
    samServer = "https://sam.test.org:443"
    petTokenCacheEnabled = true
    petTokenCacheExpiryTime = 60 minutes
    petTokenCacheMaxSize = 100
    providerTimeout = 1 second
    notebookAuthCacheEnabled = true
    notebookAuthCacheMaxSize  = 100
    notebookAuthCacheExpiryTime = 60 minutes
  }

  alwaysYesProviderConfig = {
    CreateRuntime = true
    GetRuntimeStatus = true
    ModifyRuntime = true
    ConnectToRuntime = true
    SyncDataToRuntime = true
    DeleteRuntime = true
    StopStartRuntime = true
    CreatePersistentDisk = true
    ReadPersistentDisk = true
    AttachPersistentDisk = true
    ModifyPersistentDisk = true
    DeletePersistentDisk = true
  }

  alwaysNoProviderConfig = {
    CreateRuntime = false
    ModifyRuntime = false
    GetRuntimeStatus = false
    ConnectToRuntime = false
    SyncDataToRuntime = false
    DeleteRuntime = false
    StopStartRuntime = false
    CreatePersistentDisk = false
    ReadPersistentDisk = false
    AttachPersistentDisk = false
    ModifyPersistentDisk = false
    DeletePersistentDisk = false
  }

  readOnlyProviderConfig = {
    CreateRuntime = false
    ModifyRuntime = false
    GetRuntimeStatus = true
    ConnectToRuntime = false
    SyncDataToRuntime = false
    DeleteRuntime = false
    StopStartRuntime = false
    CreatePersistentDisk = false
    ReadPersistentDisk = true
    AttachPersistentDisk = false
    ModifyPersistentDisk = false
    DeletePersistentDisk = false
  }

  syncOnlyProviderConfig = {
    CreateRuntime = false
    ModifyRuntime = false
    GetRuntimeStatus = true
    ConnectToRuntime = false
    SyncDataToRuntime = true
    DeleteRuntime = false
    StopStartRuntime = false
    CreatePersistentDisk = false
    ReadPersistentDisk = true
    AttachPersistentDisk = true
    ModifyPersistentDisk = false
    DeletePersistentDisk = false
  }

  optimizedListClustersConfig = {
    canSeeResourcesInAllProjects = false
    canSeeAllResourcesIn = ["visible-project"]
    GetRuntimeStatus = true

    #don't care about these for this test
    CreateRuntime = false
    ConnectToRuntime = false
    SyncDataToRuntime = false
    DeleteRuntime = false
    StopStartRuntime = false
    CreatePersistentDisk = false
    ReadPersistentDisk = false
    AttachPersistentDisk = false
    ModifyPersistentDisk = false
    DeletePersistentDisk = false
  }
}

serviceAccounts {
  providerConfig = {
    leoServiceAccountEmail = "leo@leo.loe"
    leoServiceAccountPemFile = "test.pem"
    providerTimeout = 1 second
  }
  kubeConfig {
    leoServiceAccountJsonFile = "placeholder.json"
    leoServiceAccountEmail = "placeholder@placeholder.com"
  }
}

autoFreeze {
  enableAutoFreeze = true
  autoFreezeAfter = 15 minutes
  autoFreezeCheckScheduler = 2 second
}

autoDelete {
  enableAutoDelete= true
  autoDeleteCheckInterval = 1 second
}

zombieRuntimeMonitor {
  enableZombieRuntimeMonitor = true
  pollPeriod = 1 second
  creationHangTolerance = 9 second
  deletionConfirmationLabelKey = "deletionConfirmed"
  concurrency = 100
}

clusterToolMonitor {
  pollPeriod = 1 second
}

clusterDnsCache {
  cacheExpiryTime = 2 seconds
  cacheMaxSize = 100
}

leoExecutionMode {
  backLeo = true
}
akka.ssl-config {
  trustManager = {
    stores = [
      {
        type = "PEM"
        path = "http/src/test/resources/test-rootCA.pem"
      }
    ]
  }
}

azure {
   pubsub-handler {
        create-vm-poll-config {
           initial-delay = 1 seconds
           max-attempts = 10
           interval = 1 seconds
       }
       delete-vm-poll-config {
           initial-delay = 1 seconds
           max-attempts = 20
           interval = 1 seconds
       }
       delete-disk-poll-config {
          initial-delay = 1 seconds
          max-attempts = 10
          interval = 1 seconds
       }
       runtime-defaults {
        vm-credential {
           username = "username"
           password = "password"
        }
       }
   }

   wsm {
      uri = "https://localhost:8000"
   }

  # We need the leo azure entity for this
   app-registration {
     client-id = ""
     client-secret = ""
     managed-app-tenant-id = ""
   }
    coa-app-config {
     instrumentation-enabled = false
   }

   # for unit test
   hail-batch-app-config {
     enabled = false
   }
}

opencensus-scala.trace.sampling-probability = 1.0

persistent-disk.dont-clone-from-these-google-folders = ["bogus"]

gke {
  allowedApp {
    sasContainerRegistry {
        sasRegistryUsername = "sasUserName"
        sasRegistryPassword = "sasPassword"
    }
  }
}