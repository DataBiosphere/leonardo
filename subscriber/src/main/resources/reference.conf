application {
  app-name = "leonardo"
  service-account-file = "replace_me"
  google-project = "replace-me"
}

mysql {
  profile = "slick.jdbc.MySQLProfile$"
  batchSize = 2000
  db {
    driver = "com.mysql.cj.jdbc.Driver"
    #url = "jdbc:mysql://mysql/leonardo
    #user = "dbUser"
    #password = "pass"
    connectionTimeout = 5000
    numThreads = 200
  }

  liquibase {
    changelog = "org/broadinstitute/dsde/workbench/leonardo/liquibase/changelog.xml"
    init-with-liquibase = true
  }
  concurrency = 120
}

auth-provider-config {
  leo-service-account-json-file = ${application.service-account-file}
  sam-uri = "replace_me"
  pet-cache-enabled = true
  pet-cache-expiry-time = 60 minutes
  pet-cache-max-size = 1000
}

cryptomining-publisher-config {
  path-to-credential-json = ${application.service-account-file}
  project-topic-name = {
    project = ${application.google-project}
    topic = "terra-cryptomining"
  }
}

async-task-processor {
  queue-bound = 500
  max-concurrent-tasks = 200
}

non-leonardo-message-subscriber {
  path-to-credential-json = ${application.service-account-file}
  topic-name = {
    project = ${application.google-project}
    topic = "replace-me"
  }
  subscription-name {
    project = ${application.google-project}
    subscription-name = "replace-me"
  }
  ack-dead-line = 5 minutes
  dead-letter-policy {
    topic-name {
      project = ${application.google-project}
      topic = "leoDeadLetterTopic"
    }
    max-retries = 5
  }
  filter = "NOT attributes:leonardo"
}

gke {
  cluster {
    location = "us-central1-a",
    region = "us-central1",
    #taken from https://dsp-security.broadinstitute.org/cloud-security/google-cloud-platform/gke
    authorized-networks = [
      "69.173.127.0/25",
      "69.173.124.0/23",
      "69.173.126.0/24",
      "69.173.127.230/31",
      "69.173.64.0/19",
      "69.173.127.224/30",
      "69.173.127.192/27",
      "69.173.120.0/22",
      "69.173.127.228/32",
      "69.173.127.232/29",
      "69.173.127.128/26",
      "69.173.96.0/20",
      "69.173.127.240/28",
      "69.173.112.0/21"
    ]
    # See https://cloud.google.com/kubernetes-engine/docs/release-notes
    # As of 2020-10-09, 1.16.x is the default but it does not work with the Galaxy chart.
    version = "1.16.15-gke.6000"
    nodepool-lock-cache-expiry-time = 1 hour
    nodepool-lock-cache-max-size = 200
  }
  ingress {
    namespace = "nginx"
    release = "nginx"
    # TODO this chart is deprecated, switch to ingress-nginx. See:
    #  https://broadworkbench.atlassian.net/browse/IA-2358
    #  https://github.com/helm/charts/tree/master/stable/nginx-ingress
    chart-name = "center/stable/nginx-ingress"
    chart-version = "1.41.3"
    load-balancer-service = "nginx-nginx-ingress-controller"
    values = [
      "rbac.create=true",
      "controller.publishService.enabled=true"
    ]
    secrets = [
      {
        name = "ca-secret"
        secret-files = [
          {
            name = "ca.crt"
            path = ${security-files.proxy-root-ca-pem}
          }
        ]
      },
      {
        name = "tls-secret"
        secret-files = [
          {
            name = "tls.crt"
            path = ${security-files.proxy-server-crt}
          },
          {
            name = "tls.key"
            path = ${security-files.proxy-server-key}
          }
        ]
      }
    ]
  }
  galaxy-app {
    # See comment in KubernetesServiceInterp for more context. Theoretically release names should
    # only need to be unique within a namespace, but something in the Galaxy chart requires them
    # to be unique within a cluster.
    release-name-suffix = "gxy-rls"
    chart-name = "galaxy/galaxykubeman"
    chart-version = "0.7.2"
    namespace-name-suffix = "gxy-ns"
    service-account-name = "gxy-ksa"
    # Setting uninstallKeepHistory will cause the `helm uninstall` command to keep a record of
    # the deleted release, which can make debugging and auditing easier. It should be safe for
    # our use case because we generate unique release names anyway.
    # See https://helm.sh/docs/intro/using_helm/#helm-uninstall-uninstalling-a-release
    uninstall-keep-history = true
    services = [
      {
        name = "galaxy"
        kind = "ClusterIP"
      }
    ]
    # Templated by firecloud-develop
    orch-url = "replace-me"
    drs-url = "replace-me"
  }
  galaxy-disk {
    nfs-persistence-name = "nfs-disk"
    postgres-persistence-name = "postgres-disk"
    # TODO: remove post-alpha once persistence is in place for Galaxy
    postgres-disk-name-suffix = "gxy-postres-disk"
    postgres-disk-size-gb = 10
    postgres-disk-block-size = 4096
  }
}

vpc {
  high-security-project-network-label = "vpc-network-name"
  high-security-project-subnetwork-label = "vpc-subnetwork-name"
  network-name = "leonardo-network"
  network-tag = "leonardo"
  # Using manual subnet creation mode because we currently only care about 1 region (us-central1)
  # and this allows us to have more control over address space. If/when we support multiple regions
  # consider auto-mode subnet creation.
  # See: https://cloud.google.com/vpc/docs/vpc#auto-mode-considerations
  auto-create-subnetworks = false
  # Note the below 3 fields are not used if autoCreateSubnetworks is true
  subnetwork-name = "leonardo-subnetwork"
  subnetwork-region = "us-central1"
  subnetwork-ip-range = "10.1.0.0/20"
  firewalls-to-add = [
    # Allows Leonardo proxy traffic on port 443
    {
      name = "leonardo-allow-https"
      source-ranges = ["0.0.0.0/0"]
      allowed = [
        {
          protocol = "tcp"
          port = "443"
        }
      ]
    },
    # Allows traffic via internal IP
    # This is a requirement for Dataproc nodes to be able to communicate with each other within a cluster.
    {
      name = "leonardo-allow-internal"
      source-ranges = ["10.1.0.0/20"]
      allowed = [
        {
          protocol = "tcp"
          port = "0-65535"
        },
        {
          protocol = "udp"
          port = "0-65535"
        },
        {
          protocol = "icmp"
        }
      ]
    },
    # Allows SSH access from the Broad network or VPN
    # IP list obtained from https://docs.google.com/document/d/1adV0LC2f_GIpl3A1AeoQuNiwcP59NToIt6VYT3xRCkU/edit
    {
      name = "leonardo-allow-broad-ssh"
      source-ranges= ${gke.cluster.authorized-networks}
      allowed = [
        {
          protocol = "tcp"
          port = 22
        }
      ]
    }
  ]
  # Remove RDP and SSH rules in the default network. Also remove legacy leonardo-notebooks-rule if it exists.
  firewalls-to-remove = ["default-allow-rdp", "default-allow-icmp", "allow-icmp"]
  poll-period = 5 seconds
  max-attempts = 24 # 2 minutes
}

proxy {
  # Should match the proxy wildcard cert
  proxy-domain = "replace_me"
  proxy-url-base = "replace_me"
  proxy-port = 443
  dns-poll-period = 15 seconds
  token-cache-expiry-time = 60 minutes
  token-cache-max-size = 20000
  internal-id-cache-expiry-time = 2 minutes
  internal-id-cache-max-size = 20000
}

dns-cache {
  kubernetes {
    cache-expiry-time = 5 seconds
    cache-max-size = 10000
  }
}

security-files {
  proxy-server-crt = "/etc/jupyter-server.crt"
  proxy-server-key = "/etc/jupyter-server.key"
  proxy-root-ca-pem = "/etc/rootCA.pem"
  proxy-root-ca-key = "/etc/rootCA.key"
  rstudio-license-file = "/etc/rstudio-license-file.lic"
}

monitor {
  kubernetes {
    create-nodepool {
      max-attempts = 90 # 10 seconds * 90 is 15 min
      interval = 10 seconds
    }
    delete-nodepool {
      max-attempts = 90 # 10 seconds * 90 is 15 min
      interval = 10 seconds
    }
    create-cluster {
      max-attempts = 120 # 15 seconds * 120 is 30 min
      interval = 15 seconds
    }
    delete-cluster {
      max-attempts = 120 # 15 seconds * 120 is 30 min
      interval = 15 seconds
    }
    create-ingress {
      max-attempts = 100 # 3 seconds * 100 is 5 min
      interval = 3 seconds
    }
    create-app {
      interval = 10 seconds
      max-attempts = 120 # 10 seconds * 120 = 20 min
    }
    delete-app {
      interval = 10 seconds
      max-attempts = 120 # 10 seconds * 120 = 20 min
    }
    scale-nodepool {
      max-attempts = 90 # 10 seconds * 90 is 15 min
      interval = 10 seconds
    }
    set-nodepool-autoscaling {
      max-attempts = 90 # 10 seconds * 90 is 15 min
      interval = 10 seconds
    }
    start-app {
      max-attempts = 100 # 3 seconds * 100 is 5 min
      interval = 3 seconds
    }
  }
}