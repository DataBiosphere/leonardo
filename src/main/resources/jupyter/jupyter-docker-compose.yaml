version: '2'
services:
  jupyter:
    container_name: "${JUPYTER_SERVER_NAME}"
    image: "${JUPYTER_DOCKER_IMAGE}"
    # Override entrypoint with a placeholder to keep the container running indefinitely.
    # The cluster init script will run some scripts as root and then start pyspark as
    # jupyter-user via docker exec.
    entrypoint: "tail -f /dev/null"
    network_mode: host
    volumes:
      # shared with welder
      - /work:/home/jupyter-user/notebooks
      - /usr/lib/bigtop-utils:/usr/lib/bigtop-utils
      - /usr/lib/hadoop:/usr/lib/hadoop
      - /usr/lib/hadoop-hdfs:/usr/lib/hadoop-hdfs
      - /usr/lib/hadoop-mapreduce:/usr/lib/hadoop-mapreduce
      - /usr/lib/hadoop-yarn:/usr/lib/hadoop-yarn
      - /usr/lib/hive:/usr/lib/hive
      - /etc/hadoop:/etc/hadoop
      - /usr/lib/spark:/usr/lib/spark
      - /etc/spark:/etc/spark
      - /etc/hive:/etc/hive
      - /usr/bin/pyspark:/usr/bin/pyspark
      - /hadoop:/hadoop
      - /hadoop_gcs_connector_metadata_cache:/hadoop_gcs_connector_metadata_cache
    restart: always
    environment:
      GOOGLE_PROJECT: "${GOOGLE_PROJECT}"
      WORKSPACE_NAMESPACE: "${GOOGLE_PROJECT}"
      CLUSTER_NAME: "${CLUSTER_NAME}"
      OWNER_EMAIL: "${OWNER_EMAIL}"
    env_file:
      - /etc/google_application_credentials.env
